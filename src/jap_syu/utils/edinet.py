"""
EDINET API ìœ í‹¸ë¦¬í‹°
ì¼ë³¸ ê¸ˆìœµì²­ì˜ ê¸°ì—…ì •ë³´ê³µê°œì‹œìŠ¤í…œ(EDINET) APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì—… ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

import httpx
import asyncio
import zipfile
import io
import re
import os
import json
from typing import Dict, List, Optional, Tuple, NamedTuple
from datetime import datetime, date, timedelta
from dataclasses import dataclass
from pathlib import Path
from loguru import logger
from bs4 import BeautifulSoup
from ..models import EdinetData, EdinetBasic, EdinetHR, EdinetFinancials, IRDocument

# .env íŒŒì¼ ë¡œë“œ (python-dotenvê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenvê°€ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë§Œ ì‚¬ìš©
    pass

# EDINET API ì„¤ì •
EDINET_API_BASE = "https://api.edinet-fsa.go.jp/api/v2"
EDINET_API_KEY = os.getenv("EDINET_API_KEY", "your-api-key-here")

@dataclass
class CompanyDocument:
    """ë°œê²¬ëœ ê¸°ì—… ë¬¸ì„œ ì •ë³´"""
    document_id: str
    company_name: str 
    submitted_date: str
    doc_type: str
    company_key: str  # ë‚´ë¶€ ì‹ë³„ìš©

class EdinetAPI:
    """EDINET API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.base_url = EDINET_API_BASE
        self.session = None
    
    async def __aenter__(self):
        self.session = httpx.AsyncClient(timeout=30.0)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.aclose()
    
    async def get_document_list(self, date: str) -> List[dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        try:
            response = await self.session.get(
                f"{self.base_url}/documents.json",
                params={"date": date, "type": 2},  # type=2: ë©”íƒ€ë°ì´í„°ë§Œ
                headers={"Ocp-Apim-Subscription-Key": EDINET_API_KEY}
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("results", [])
            else:
                logger.warning(f"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ ({date}): {response.status_code}")
                
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ({date}): {e}")
        
        return []

    async def get_document_package(self, document_id: str, doc_type: str = "1") -> Optional[bytes]:
        """ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (type=1: ZIP íŒŒì¼)"""
        try:
            response = await self.session.get(
                f"{self.base_url}/documents/{document_id}",
                params={"type": doc_type},  # 1: ZIP íŒŒì¼ (iXBRL í¬í•¨)
                headers={"Ocp-Apim-Subscription-Key": EDINET_API_KEY}
            )
            
            if response.status_code == 200:
                return response.content
            else:
                logger.error(f"EDINET ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"EDINET ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_honbun_files(self, zip_content: bytes) -> List[Tuple[str, str]]:
        """ZIP íŒŒì¼ì—ì„œ honbun iXBRL íŒŒì¼ë“¤ ì¶”ì¶œ"""
        honbun_files = []
        
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                for file_info in zip_file.filelist:
                    filename = file_info.filename
                    if "honbun" in filename and ("ixbrl" in filename or "ix:" in filename):
                        content = zip_file.read(filename).decode('utf-8', errors='ignore')
                        honbun_files.append((filename, content))
                        logger.info(f"honbun íŒŒì¼ ë°œê²¬: {filename}")
        
        except Exception as e:
            logger.error(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return honbun_files
    
    def parse_employee_info(self, honbun_files: List[Tuple[str, str]]) -> Dict[str, any]:
        """honbun íŒŒì¼ë“¤ì—ì„œ ì¸ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        employee_info = {
            "avgTenureYears": None,
            "avgAgeYears": None,
            "avgAnnualSalaryJPY": None,
            "employeeCount": None,
            "provenance": {}
        }
        
        # ì°¾ì„ í‚¤ì›Œë“œë“¤
        keywords = {
            "avgTenureYears": ["å¹³å‡å‹¤ç¶šå¹´æ•°", "AverageLengthOfServiceYears"],
            "avgAgeYears": ["å¹³å‡å¹´é½¢", "AverageAgeYears"],
            "avgAnnualSalaryJPY": ["å¹³å‡å¹´é–“çµ¦ä¸", "AverageAnnualSalary"],
            "employeeCount": ["å¾“æ¥­å“¡æ•°", "NumberOfEmployees"]
        }
        
        # iXBRL ê°œë… ì´ë¦„ë“¤ (ì‹¤ì œ name ì†ì„±ê°’)
        ixbrl_concepts = {
            "avgTenureYears": [
                "jpcrp_cor:AverageLengthOfServiceYearsInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageLengthOfServiceYearsInformationOfReportingCompanyInformation",
                "jpcrp_cor:AverageLengthOfServiceYears"
            ],
            "avgAgeYears": [
                "jpcrp_cor:AverageAgeYearsInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageAgeYearsInformationOfReportingCompanyInformation", 
                "jpcrp_cor:AverageAgeYears",
                "jpcrp_cor:AverageAge"
            ],
            "avgAnnualSalaryJPY": [
                "jpcrp_cor:AverageAnnualSalaryInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageAnnualSalaryInformationOfReportingCompanyInformation",
                "jpcrp_cor:AverageAnnualSalary"
            ],
            "employeeCount": [
                "jpcrp_cor:NumberOfEmployees",
                "jpcrp_cor:AverageNumberOfEmployees"
            ]
        }
        
        for filename, content in honbun_files:
            # å¾“æ¥­å“¡ã®çŠ¶æ³ ì„¹ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
            if "å¾“æ¥­å“¡ã®çŠ¶æ³" not in content:
                continue
            
            logger.info(f"å¾“æ¥­å“¡ã®çŠ¶æ³ ì„¹ì…˜ ë°œê²¬: {filename}")
            
            # ê° ì§€í‘œë³„ë¡œ ì¶”ì¶œ
            for field in keywords.keys():
                if employee_info[field] is not None:
                    continue  # ì´ë¯¸ ì°¾ì•˜ìœ¼ë©´ ìŠ¤í‚µ
                
                # 1. ìš°ì„  iXBRL ê°œë…ìœ¼ë¡œ ê²€ìƒ‰ (ê°€ì¥ ì •í™•)
                if field in ixbrl_concepts:
                    for concept_name in ixbrl_concepts[field]:
                        results = self._extract_value_from_ixbrl_concept(content, concept_name, filename)
                        if results:
                            # contextRefë¡œ ìµœì  ê°’ ì„ íƒ
                            best_result = self._select_best_value_by_context(results)
                            # scale/decimals/unitRef ì†ì„± ì ìš©í•˜ì—¬ ìµœì¢… ê°’ ê³„ì‚°
                            final_value = self._apply_ixbrl_attributes(best_result, field)
                            employee_info[field] = final_value
                            employee_info["provenance"][field] = {
                                "file": filename,
                                "concept": concept_name,
                                "contextRef": best_result['contextRef'],
                                "unitRef": best_result['unitRef'],
                                "scale": best_result['scale'],
                                "method": "ixbrl_concept"
                            }
                            logger.info(f"{field}: iXBRL ê°œë…ìœ¼ë¡œ ê°’ ì¶”ì¶œ ì„±ê³µ - {best_result['value']}")
                            break
                
                # 2. iXBRLë¡œ ëª» ì°¾ìœ¼ë©´ í‚¤ì›Œë“œ ë°±ì—…
                if employee_info[field] is None:
                    for keyword in keywords[field]:
                        if keyword in content:
                            value = self._extract_value_from_context(content, keyword, filename)
                            if value is not None:
                                employee_info[field] = value
                                employee_info["provenance"][field] = {
                                    "file": filename,
                                    "keyword": keyword,
                                    "method": "text_search_backup"
                                }
                                logger.info(f"{field}: í…ìŠ¤íŠ¸ ë°±ì—…ìœ¼ë¡œ ê°’ ì¶”ì¶œ - {value}")
                                break
        
        return employee_info
    
    def _extract_value_from_context(self, content: str, keyword: str, filename: str) -> Optional[float]:
        """í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ìˆ˜ì¹˜ ì¶”ì¶œ"""
        try:
            # ì¢…ì—…ì›ìˆ˜ì˜ ê²½ìš° ë‹¨ìˆœíˆ ê°’ë§Œ ì¶”ì¶œ
            if "å¾“æ¥­å“¡æ•°" in keyword or "NumberOfEmployees" in keyword:
                return self._extract_employee_count(content, filename)
            
            # í‰ê·  ì—°ë´‰ì˜ ê²½ìš° ì •í™•í•œ ì› ë‹¨ìœ„ ê°’ ì°¾ê¸°  
            if "å¹³å‡å¹´é–“çµ¦ä¸" in keyword or "AverageAnnualSalary" in keyword:
                return self._extract_annual_salary(content, filename)
            
            # ê¸°íƒ€ í‚¤ì›Œë“œë“¤ (ê·¼ì†ì—°ìˆ˜, ë‚˜ì´)
            if "å¹³å‡å‹¤ç¶šå¹´æ•°" in keyword:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)\s*å¹´"
            elif "å¹³å‡å¹´é½¢" in keyword:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)\s*æ­³"
            else:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)"
            
            match = re.search(pattern, content)
            if match:
                raw_value = match.group(1).replace(',', '')
                value = float(raw_value)
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì—ì„œ ê°’ ì¶”ì¶œ: {value} (íŒŒì¼: {filename})")
                return value
        except Exception as e:
            logger.error(f"ê°’ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _select_best_value_by_context(self, results: List[dict]) -> dict:
        """contextRef ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ ê°’ ì„ íƒ"""
        if not results:
            return None
            
        if len(results) == 1:
            return results[0]
        
        logger.info(f"ì—¬ëŸ¬ ê°’ ì¤‘ ìµœì  ì„ íƒ - ì´ {len(results)}ê°œ:")
        for i, result in enumerate(results):
            logger.info(f"  {i+1}: {result['value']} (contextRef: {result['contextRef']})")
        
        # ì„ íƒ ê¸°ì¤€:
        # 1. ì „ì²´/ì—°ê²° ê¸°ì¤€ ìš°ì„  (ì„¸ê·¸ë¨¼íŠ¸ ì œì™¸)
        # 2. ê°€ì¥ ìµœê·¼ ì—°ë„ (Current > Prior1 > Prior2...)
        # 3. ì—°ê²° ê¸°ì¤€ > ë‹¨ë… ê¸°ì¤€
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ ìš°ì„ ìˆœìœ„ ë¶„ë¥˜
        overall_results = []      # ì „ì²´/ì—°ê²° (ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ)
        segment_results = []      # ì„¸ê·¸ë¨¼íŠ¸ë³„ 
        non_consol_results = []   # ë‹¨ë… ê¸°ì¤€
        
        for result in results:
            context_ref = result.get('contextRef') or ''
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë©¤ë²„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ReportableSegment, CorporateShared ë“±)
            if any(segment in context_ref for segment in ['ReportableSegment', 'Member']):
                if 'NonConsolidated' in context_ref:
                    non_consol_results.append(result)
                else:
                    segment_results.append(result)
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ëŠ” ì „ì²´/ì—°ê²° ê¸°ì¤€
                overall_results.append(result)
        
        # ìš°ì„ ìˆœìœ„: ì „ì²´/ì—°ê²° > ì„¸ê·¸ë¨¼íŠ¸ > ë‹¨ë…
        candidates = overall_results if overall_results else (segment_results if segment_results else non_consol_results)
        
        if not candidates:
            candidates = results
        
        # ê°€ì¥ ìµœì  ê°’ ì„ íƒ (ì—¬ëŸ¬ ê¸°ì¤€ ì ìš©)
        best_result = candidates[0]
        
        # ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì„ íƒ
        def get_context_priority(context_ref):
            """contextRefì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜ (ë‚®ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)"""
            if 'CurrentYear' in context_ref:
                return 0  # ìµœê³  ìš°ì„ ìˆœìœ„
            elif 'Prior1Year' in context_ref:
                return 1
            elif 'Prior2Year' in context_ref:
                return 2
            elif 'Prior3Year' in context_ref:
                return 3
            elif 'Prior4Year' in context_ref:
                return 4
            else:
                return 999  # ì•Œ ìˆ˜ ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ë‚®ì€ ìš°ì„ ìˆœìœ„
        
        # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ (ìˆ«ìê°€ ê°€ì¥ ë‚®ì€) ê²°ê³¼ ì„ íƒ
        best_result = min(candidates, key=lambda x: get_context_priority(x.get('contextRef', '')))
        
        logger.info(f"ìµœì¢… ì„ íƒ: {best_result['value']} (contextRef: {best_result['contextRef']})")
        return best_result
    
    def _apply_ixbrl_attributes(self, result: dict, field: str) -> float:
        """scale/decimals/unitRef ì†ì„±ì„ ì ìš©í•˜ì—¬ ìµœì¢… ê°’ ê³„ì‚°"""
        try:
            value = float(str(result['value']).replace(',', ''))
        except (ValueError, AttributeError):
            logger.warning(f"ê°’ ë³€í™˜ ì‹¤íŒ¨: {result['value']}")
            return 0.0
        
        scale = result.get('scale')
        decimals = result.get('decimals')
        unit_ref = result.get('unitRef')
        
        logger.info(f"ì†ì„± ì ìš© ì „ - ê°’: {value}, scale: {scale}, decimals: {decimals}, unit: {unit_ref}")
        
        # scale ì ìš© (10^scaleë¥¼ ê³±í•¨)
        if scale is not None:
            try:
                scale_factor = int(scale)
                value *= (10 ** scale_factor)
                logger.info(f"scale {scale} ì ìš©: {result['value']} â†’ {value}")
            except ValueError:
                logger.warning(f"ì˜ëª»ëœ scale ê°’: {scale}")
        
        # decimals ì ìš© (ìŒìˆ˜ë©´ scale íš¨ê³¼, ì–‘ìˆ˜ë©´ ì†Œìˆ˜ì  ìë¦¿ìˆ˜)
        if decimals is not None:
            try:
                decimal_places = int(decimals)
                if decimal_places < 0:
                    # ìŒìˆ˜ decimalsëŠ” 10ì˜ ì§€ìˆ˜ë¡œ ë‚˜ëˆ” (scaleê³¼ ë¹„ìŠ·í•œ íš¨ê³¼)
                    value /= (10 ** abs(decimal_places))
                    logger.info(f"decimals {decimals} ì ìš© (scale íš¨ê³¼): {result['value']} â†’ {value}")
                elif decimal_places >= 0:
                    # ì–‘ìˆ˜ decimalsëŠ” ì†Œìˆ˜ì  ìë¦¿ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
                    value = round(value, decimal_places)
                    logger.info(f"decimals {decimals} ì ìš© (ë°˜ì˜¬ë¦¼): ì†Œìˆ˜ì  {decimal_places}ìë¦¬")
            except ValueError:
                logger.warning(f"ì˜ëª»ëœ decimals ê°’: {decimals}")
        
        # unitRef ê²€ì¦ ë° ë¡œê¹…
        if unit_ref:
            if field == 'avgAnnualSalaryJPY' and 'JPY' not in unit_ref.upper():
                logger.warning(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ í†µí™” ë‹¨ìœ„: {unit_ref} (JPY ì˜ˆìƒ)")
            logger.info(f"ë‹¨ìœ„ í™•ì¸: {unit_ref}")
        
        # í•„ë“œë³„ íƒ€ì… ë³€í™˜
        if field in ['avgAnnualSalaryJPY', 'employeeCount']:
            value = int(value)  # ì •ìˆ˜ ë³€í™˜
        else:
            value = float(value)  # ì†Œìˆ˜ì  ìœ ì§€
        
        logger.info(f"ìµœì¢… ì ìš©ëœ ê°’: {value}")
        return value
    
    def _extract_employee_count(self, content: str, filename: str) -> Optional[float]:
        """ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ (ë‹¨ìˆœ ë°©ì‹)"""
        try:
            logger.info(f"ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ ì‹œì‘ - íŒŒì¼: {filename}")
            
            # ë‹¨ìˆœ ë°©ì‹: ê´‘ë²”ìœ„í•œ íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì§ì›ìˆ˜ë¥¼ ì°¾ê³  ê°€ì¥ í° ê°’ ì„ íƒ
            patterns = [
                # ë‹¨ìˆœ ìˆ«ì + äºº íŒ¨í„´ (í° ìˆ«ìë§Œ)
                r"([0-9]{4,5}[,0-9]*)\s*äºº",
                # í…Œì´ë¸” í˜•íƒœ íŒ¨í„´  
                r"åˆè¨ˆ[^0-9]*?([0-9,]+)\s*äºº",
                r"è¨ˆ[^0-9]*?([0-9,]+)\s*äºº",
                # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤
                r"å¾“æ¥­å“¡æ•°[^0-9]*?([0-9,]+)\s*äºº",
                r"å¾“æ¥­å“¡[^0-9]*?([0-9,]+)\s*äºº",
                r"NumberOfEmployees[^0-9]*?([0-9,]+)",
            ]
            
            all_matches = []
            
            for i, pattern in enumerate(patterns):
                matches = re.findall(pattern, content, re.DOTALL)
                if matches:
                    logger.info(f"íŒ¨í„´ {i+1}ì—ì„œ ë§¤ì¹˜ ë°œê²¬: {matches[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
                    all_matches.extend(matches)
            
            if all_matches:
                # ëª¨ë“  ë§¤ì¹˜ì—ì„œ ê°€ì¥ í° ê°’ ì°¾ê¸° (í•©ê³„ê°’)
                max_value = 0
                for match in all_matches:
                    try:
                        # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        clean_match = re.sub(r'[^0-9]', '', str(match))
                        if clean_match:
                            value = float(clean_match)
                            if value > max_value and value > 1000:  # 1000ëª… ì´ìƒë§Œ
                                max_value = value
                    except:
                        continue
                
                if max_value > 0:
                    logger.info(f"ìµœì¢… ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ: {max_value} (íŒŒì¼: {filename})")
                    return max_value
            
            logger.warning(f"ì¢…ì—…ì›ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - íŒŒì¼: {filename}")
                    
        except Exception as e:
            logger.error(f"ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _estimate_data_year(self, content: str, filename: str) -> int:
        """ë°ì´í„° ì—°ë„ ì¶”ì •"""
        # 1. íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ ì‹œë„ (2025-03-31 ê°™ì€ íŒ¨í„´)
        year_match = re.search(r'(\d{4})', filename)
        if year_match:
            year = int(year_match.group(1))
            if 2020 <= year <= 2030:  # í•©ë¦¬ì ì¸ ë²”ìœ„
                logger.info(f"íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì •: {year}")
                return year
        
        # 2. ë‚´ìš©ì—ì„œ ê°€ì¥ ìµœê·¼ ì—°ë„ ì°¾ê¸°
        years = re.findall(r'(20[2-3][0-9])[å¹´\s]', content)
        if years:
            recent_year = max([int(y) for y in years if 2020 <= int(y) <= 2030])
            logger.info(f"ë‚´ìš©ì—ì„œ ì—°ë„ ì¶”ì •: {recent_year}")
            return recent_year
            
        # 3. ê¸°ë³¸ê°’: í˜„ì¬ ì—°ë„ - 1 (ë³´ê³ ì„œëŠ” ë³´í†µ ì „ë…„ë„ ê¸°ì¤€)
        default_year = datetime.now().year - 1
        logger.info(f"ê¸°ë³¸ê°’ìœ¼ë¡œ ì—°ë„ ì¶”ì •: {default_year}")
        return default_year
    
    def _find_employee_data_year(self, content: str, employee_count: float) -> int:
        """ì‹¤ì œ ì§ì›ìˆ˜ì™€ ì—°ê²°ëœ ì—°ë„ ì°¾ê¸°"""
        try:
            # ì§ì›ìˆ˜ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì½¤ë§ˆ í¬í•¨/ë¯¸í¬í•¨ ëª¨ë‘ ê³ ë ¤)
            count_str = str(int(employee_count))
            count_with_comma = f"{int(employee_count):,}"
            
            logger.info(f"ì§ì›ìˆ˜ {employee_count}ì™€ ì—°ê²°ëœ ì—°ë„ ì°¾ê¸° ì‹œì‘")
            
            # í˜„ì¬ ì‹œì ì—ì„œ í•©ë¦¬ì í•œ ì—°ë„ ë²”ìœ„ (2020~2025)
            current_year = datetime.now().year
            valid_years = range(2020, current_year + 1)
            
            for year in sorted(valid_years, reverse=True):  # ìµœê·¼ ì—°ë„ë¶€í„° ì°¾ê¸°
                # í•´ë‹¹ ì—°ë„ì™€ ì§ì›ìˆ˜ê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ë“¤ ì°¾ê¸°
                patterns = [
                    rf"{year}[å¹´\s].*?{count_str}\s*äºº",
                    rf"{year}[å¹´\s].*?{count_with_comma}\s*äºº",
                    rf"{year}.*?{count_str}\s*äºº",
                    rf"{year}.*?{count_with_comma}\s*äºº",
                    # í…Œì´ë¸” í˜•íƒœì—ì„œ ë…„ë„ì™€ ì§ì›ìˆ˜ê°€ ê°™ì€ í–‰ì— ìˆëŠ” ê²½ìš°
                    rf"{year}[^\n]*{count_str}[^\n]*äºº",
                    rf"{year}[^\n]*{count_with_comma}[^\n]*äºº"
                ]
                
                for pattern in patterns:
                    if re.search(pattern, content, re.DOTALL):
                        logger.info(f"ì§ì›ìˆ˜ {employee_count}ê°€ {year}ë…„ê³¼ ì—°ê²°ë¨")
                        return year
            
            # ì§ì ‘ì ì¸ ì—°ê²°ì´ ì—†ìœ¼ë©´ ìµœì‹  ì—°ë„ (2025ë…„ ë˜ëŠ” 2024ë…„)
            default_year = current_year  # ìµœì‹  ì—°ë„ë¡œ ì„¤ì •
            logger.info(f"ì—°ê²°ëœ ì—°ë„ ì—†ìŒ, ìµœì‹  ì—°ë„ë¡œ ì„¤ì •: {default_year}")
            return default_year
                    
        except Exception as e:
            logger.error(f"ì—°ë„ ì°¾ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            return datetime.now().year  # ìµœì‹  ì—°ë„
    
    def _extract_annual_salary(self, content: str, filename: str) -> Optional[float]:
        """í‰ê·  ì—°ë´‰ ì¶”ì¶œ (ì› ë‹¨ìœ„)"""
        try:
            # 11,453,407 ê°™ì€ ì •í™•í•œ ì› ë‹¨ìœ„ ê°’ ì°¾ê¸°
            patterns = [
                r"å¹³å‡å¹´é–“çµ¦ä¸[^0-9]*?([0-9,]+)\s*å††",  # ì¼ë³¸ì–´ + å††
                r"AverageAnnualSalary[^0-9]*?([0-9,]+)",  # ì˜ë¬¸
                r"å¹³å‡å¹´é–“çµ¦ä¸[^0-9]*?([0-9,]+)\s*ä¸‡å††", # ë§Œì› ë‹¨ìœ„
            ]
            
            for pattern in patterns:
                match = re.search(pattern, content)
                if match:
                    raw_value = match.group(1).replace(',', '')
                    value = float(raw_value)
                    
                    # ë§Œì› ë‹¨ìœ„ì¸ ê²½ìš° ì›ìœ¼ë¡œ ë³€í™˜
                    if "ä¸‡å††" in pattern:
                        value *= 10000
                    
                    logger.info(f"ì—°ë´‰ ì¶”ì¶œ: {value} (íŒŒì¼: {filename}, íŒ¨í„´: {pattern})")
                    return value
                    
        except Exception as e:
            logger.error(f"ì—°ë´‰ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _extract_value_from_ixbrl_concept(self, content: str, concept_name: str, filename: str) -> Optional[List[dict]]:
        """iXBRL ê°œë…(name ì†ì„±)ì—ì„œ ìˆ˜ì¹˜ ì¶”ì¶œ"""
        try:
            # ì „ì²´ ix:nonFraction íƒœê·¸ ë§¤ì¹­ (ì†ì„±ê³¼ ê°’ ëª¨ë‘ í¬í•¨)
            full_pattern = rf'<ix:nonFraction([^>]*name="{re.escape(concept_name)}"[^>]*)>([^<]+)</ix:nonFraction>'
            full_matches = re.findall(full_pattern, content)
            
            if full_matches:
                logger.info(f"ê°œë… '{concept_name}'ì—ì„œ {len(full_matches)}ê°œ ê°’ ë°œê²¬: {[m[1] for m in full_matches[:3]]}...")
                
                results = []
                for attrs, value_text in full_matches:
                    try:
                        # ìˆ«ì ê°’ ì¶”ì¶œ (ì½¤ë§ˆ ì œê±°)
                        clean_value = re.sub(r'[,\s]', '', value_text)
                        if not re.match(r'^-?[\d.]+$', clean_value):
                            continue
                            
                        value = float(clean_value)
                        
                        # ì†ì„±ë“¤ íŒŒì‹± (ì „ì²´ ì†ì„± ë¬¸ìì—´ì—ì„œ ì¶”ì¶œ)
                        result = {
                            'value': value,
                            'raw_text': value_text,
                            'contextRef': self._extract_attribute(attrs, 'contextRef'),
                            'unitRef': self._extract_attribute(attrs, 'unitRef'),
                            'scale': self._extract_attribute(attrs, 'scale'),
                            'decimals': self._extract_attribute(attrs, 'decimals'),
                            'concept': concept_name,
                            'file': filename
                        }
                        results.append(result)
                        
                        # ë””ë²„ê¹…ìš© ë¡œê·¸
                        logger.debug(f"ì¶”ì¶œëœ ê°’: {value}, contextRef: {result['contextRef']}, unitRef: {result['unitRef']}")
                        
                    except ValueError as e:
                        logger.debug(f"ê°’ ë³€í™˜ ì‹¤íŒ¨: {value_text} - {e}")
                        continue
                
                if results:
                    logger.info(f"ê°œë… '{concept_name}'ì—ì„œ {len(results)}ê°œ ìœ íš¨í•œ ê°’ ì¶”ì¶œ")
                    return results
                    
        except Exception as e:
            logger.error(f"iXBRL ê°œë… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _extract_attribute(self, attrs_string: str, attr_name: str) -> Optional[str]:
        """ì†ì„± ë¬¸ìì—´ì—ì„œ íŠ¹ì • ì†ì„±ê°’ ì¶”ì¶œ"""
        try:
            pattern = rf'{attr_name}="([^"]*)"'
            match = re.search(pattern, attrs_string)
            return match.group(1) if match else None
        except:
            return None

def parse_edinet_basic_info(company_code: str) -> EdinetBasic:
    """EDINET ê¸°ì—… ê¸°ë³¸ ì •ë³´ íŒŒì‹± (í˜„ì¬ëŠ” ì½”ë“œë§Œ ì €ì¥)"""
    basic = EdinetBasic()
    basic.name = "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹"  # ê¸°ë³¸ê°’
    basic.address = "æ±äº¬éƒ½åƒä»£ç”°åŒºä¸¸ã®å†…1-9-1"  # ê¸°ë³¸ê°’
    return basic

def parse_edinet_financials(company_code: str) -> EdinetFinancials:
    """EDINET ì¬ë¬´ ì •ë³´ íŒŒì‹± (í˜„ì¬ëŠ” ê¸°ë³¸ê°’)"""
    financials = EdinetFinancials()
    financials.fiscalYear = datetime.now().year
    return financials

class CompanyReportUpdater:
    """ê¸°ì—… ë¦¬í¬íŠ¸ ìµœì‹ í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        # íƒ€ê²Ÿ ê¸°ì—…ë“¤ê³¼ ë§¤ì¹­ í‚¤ì›Œë“œ (ëª¨ë‘ ë³¸ì‚¬ë§Œ ì •í™•íˆ ë§¤ì¹­)
        self.target_companies = {
            "rakuten": ["æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"],
            "mercari": ["æ ªå¼ä¼šç¤¾ãƒ¡ãƒ«ã‚«ãƒª"],  
            "cyberagent": ["æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"],
            "lineyahoo": ["ï¼¬ï¼©ï¼®ï¼¥ãƒ¤ãƒ•ãƒ¼æ ªå¼ä¼šç¤¾"],
            "recruit": ["æ ªå¼ä¼šç¤¾ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹"],
            "dena": ["æ ªå¼ä¼šç¤¾ãƒ‡ã‚£ãƒ¼ãƒ»ã‚¨ãƒŒãƒ»ã‚¨ãƒ¼"],
            "sony": ["ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"],
            "softbank": ["ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"],
            "fujitsu": ["å¯Œå£«é€šæ ªå¼ä¼šç¤¾"],  # ë³¸ì‚¬ë¡œ ë³€ê²½
            "nttdata": ["æ ªå¼ä¼šç¤¾ï¼®ï¼´ï¼´ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—"]
        }
        
        # ìƒíƒœ ì €ì¥ íŒŒì¼ ê²½ë¡œ
        self.state_file = Path("data/last_check_dates.json")
        self.discovered_reports = {}  # company_key -> ìµœì‹  ë¬¸ì„œ ì •ë³´
    
    def load_last_check_dates(self) -> Dict[str, str]:
        """ë§ˆì§€ë§‰ ì²´í¬ ë‚ ì§œ ë¡œë“œ"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ê°’: 18ê°œì›” ì „ë¶€í„° ì‹œì‘
        default_date = (datetime.now() - timedelta(days=18*30)).strftime("%Y-%m-%d")
        return {company_key: default_date for company_key in self.target_companies.keys()}
    
    def save_last_check_dates(self, dates: Dict[str, str]):
        """ë§ˆì§€ë§‰ ì²´í¬ ë‚ ì§œ ì €ì¥"""
        try:
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.state_file, 'w') as f:
                json.dump(dates, f, indent=2)
        except Exception as e:
            logger.error(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def match_company(self, company_name: str) -> Optional[str]:
        """íšŒì‚¬ëª…ìœ¼ë¡œ íƒ€ê²Ÿ ê¸°ì—… ë§¤ì¹­"""
        for company_key, keywords in self.target_companies.items():
            for keyword in keywords:
                if keyword in company_name:
                    return company_key
        return None
    
    def date_range(self, start_date: datetime, end_date: datetime):
        """ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì—­ìˆœ)"""
        current_date = end_date
        while current_date >= start_date:
            yield current_date.strftime("%Y-%m-%d")
            current_date -= timedelta(days=1)
    
    async def scan_date_for_reports(self, date: str, api: EdinetAPI) -> List[CompanyDocument]:
        """íŠ¹ì • ë‚ ì§œì—ì„œ íƒ€ê²Ÿ ê¸°ì—…ë“¤ì˜ ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ê²€ìƒ‰"""
        documents = await api.get_document_list(date)
        found_reports = []
        
        for doc in documents:
            # ìœ ê°€ì¦ê¶Œë³´ê³ ì„œë§Œ (docTypeCode: 120)
            if doc.get("docTypeCode") != "120":
                continue
            
            company_name = doc.get("filerName", "")
            company_key = self.match_company(company_name)
            
            if company_key:
                report = CompanyDocument(
                    document_id=doc.get("docID"),
                    company_name=company_name,
                    submitted_date=date,
                    doc_type="120",
                    company_key=company_key
                )
                found_reports.append(report)
                logger.info(f"ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ë°œê²¬: {company_name} ({date}) - {doc.get('docID')}")
        
        return found_reports
    
    async def find_latest_reports(self, months_back: int = 18) -> Dict[str, CompanyDocument]:
        """ìµœì‹  ìœ ê°€ì¦ê¶Œë³´ê³ ì„œë“¤ ê²€ìƒ‰"""
        logger.info(f"ìµœê·¼ {months_back}ê°œì›”ê°„ ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ê²€ìƒ‰ ì‹œì‘...")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        end_date = datetime.now()
        start_date = end_date - timedelta(days=months_back * 30)
        
        # ê° íšŒì‚¬ë³„ ìµœì‹  ë¦¬í¬íŠ¸ ì €ì¥
        latest_reports = {}
        dates_scanned = 0
        
        async with EdinetAPI() as api:
            # ë‚ ì§œë³„ ìŠ¤ìº” (ìµœì‹  ë‚ ì§œë¶€í„°)
            for date_str in self.date_range(start_date, end_date):
                dates_scanned += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if dates_scanned % 30 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {dates_scanned}ì¼ ìŠ¤ìº” ì™„ë£Œ ({date_str})")
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë¦¬í¬íŠ¸ë“¤ ê²€ìƒ‰
                found_reports = await self.scan_date_for_reports(date_str, api)
                
                # ê° íšŒì‚¬ë³„ë¡œ ê°€ì¥ ìµœê·¼ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸
                for report in found_reports:
                    company_key = report.company_key
                    
                    # ë” ìµœì‹  ë¦¬í¬íŠ¸ì´ê±°ë‚˜ ì²˜ìŒ ë°œê²¬í•œ ê²½ìš°
                    if (company_key not in latest_reports or 
                        report.submitted_date > latest_reports[company_key].submitted_date):
                        
                        latest_reports[company_key] = report
                        logger.info(f"âœ¨ {company_key} ìµœì‹  ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸: {report.submitted_date}")
                
                # ëª¨ë“  íšŒì‚¬ì˜ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                if len(latest_reports) == len(self.target_companies):
                    logger.info(f"ğŸ‰ ëª¨ë“  íšŒì‚¬ì˜ ë¦¬í¬íŠ¸ ë°œê²¬! {dates_scanned}ì¼ ìŠ¤ìº”ìœ¼ë¡œ ì™„ë£Œ")
                    break
                
                # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                await asyncio.sleep(0.1)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info(f"\nğŸ“Š ìµœì‹ í™” ê²°ê³¼ ìš”ì•½:")
        logger.info(f"- ìŠ¤ìº” ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"- ìŠ¤ìº” ì¼ìˆ˜: {dates_scanned}ì¼")
        logger.info(f"- ë°œê²¬í•œ íšŒì‚¬: {len(latest_reports)}/{len(self.target_companies)}ê°œ")
        
        for company_key, report in latest_reports.items():
            logger.info(f"  â€¢ {company_key}: {report.company_name} ({report.submitted_date})")
        
        # ëª» ì°¾ì€ íšŒì‚¬ë“¤
        missing_companies = set(self.target_companies.keys()) - set(latest_reports.keys())
        if missing_companies:
            logger.warning(f"âŒ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ íšŒì‚¬ë“¤: {missing_companies}")
        
        return latest_reports
    
    async def update_company_data(self, company_key: str, document: CompanyDocument) -> bool:
        """íŠ¹ì • íšŒì‚¬ì˜ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        logger.info(f"ğŸ”„ {company_key} ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        try:
            async with EdinetAPI() as api:
                # ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
                zip_content = await api.get_document_package(document.document_id, doc_type="1")
                
                if zip_content:
                    # honbun íŒŒì¼ ì¶”ì¶œ ë° íŒŒì‹±
                    honbun_files = api.extract_honbun_files(zip_content)
                    
                    if honbun_files:
                        employee_info = api.parse_employee_info(honbun_files)
                        
                        # EdinetData ê°ì²´ ìƒì„±
                        edinet_data = EdinetData()
                        
                        # ê¸°ë³¸ ì •ë³´ ì„¤ì •
                        edinet_data.basic = EdinetBasic()
                        edinet_data.basic.name = document.company_name
                        edinet_data.basic.employee_count = employee_info.get("employeeCount")
                        
                        # HR ì •ë³´ ì„¤ì •
                        edinet_data.hr.avgTenureYears = employee_info.get("avgTenureYears")
                        edinet_data.hr.avgAgeYears = employee_info.get("avgAgeYears")
                        edinet_data.hr.avgAnnualSalaryJPY = employee_info.get("avgAnnualSalaryJPY")
                        
                        # ì¬ë¬´ ì •ë³´ ì„¤ì •
                        edinet_data.financials = EdinetFinancials()
                        edinet_data.financials.fiscalYear = datetime.now().year
                        
                        # ì¶œì²˜ ì •ë³´ ì„¤ì •
                        edinet_data.provenance = {
                            "source": "EDINET API v2",
                            "document_id": document.document_id,
                            "submitted_date": document.submitted_date,
                            "fetched_at": datetime.now().isoformat(),
                            "company_key": company_key,
                            "employee_info_provenance": employee_info.get("provenance", {})
                        }
                        
                        # ë°ì´í„° ì €ì¥
                        await self.save_company_data(company_key, edinet_data)
                        
                        logger.info(f"âœ… {company_key} ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                        return True
                    else:
                        logger.error(f"âŒ {company_key} honbun íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨")
                else:
                    logger.error(f"âŒ {company_key} ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    
        except Exception as e:
            logger.error(f"âŒ {company_key} ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    async def save_company_data(self, company_key: str, edinet_data: EdinetData):
        """íšŒì‚¬ ë°ì´í„° ì €ì¥"""
        output_path = Path(f"data/edinet_reports/{company_key}.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # EdinetDataë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(edinet_data.model_dump(), f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ {company_key} ë°ì´í„° ì €ì¥: {output_path}")
    
    async def run_full_update(self) -> Dict[str, bool]:
        """ì „ì²´ ìµœì‹ í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸš€ EDINET ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ìµœì‹ í™” ì‹œì‘...")
        
        # 1. ìµœì‹  ë¦¬í¬íŠ¸ë“¤ ê²€ìƒ‰
        latest_reports = await self.find_latest_reports()
        
        if not latest_reports:
            logger.warning("ê²€ìƒ‰ëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2. ê° íšŒì‚¬ë³„ ë°ì´í„° ì—…ë°ì´íŠ¸
        update_results = {}
        
        for company_key, document in latest_reports.items():
            logger.info(f"\nğŸ“‹ {company_key} ì²˜ë¦¬ ì¤‘...")
            success = await self.update_company_data(company_key, document)
            update_results[company_key] = success
            
            # API ë¶€í•˜ ë°©ì§€
            await asyncio.sleep(2.0)
        
        # 3. ê²°ê³¼ ìš”ì•½
        successful_updates = sum(update_results.values())
        logger.info(f"\nğŸŠ ìµœì‹ í™” ì™„ë£Œ!")
        logger.info(f"ì„±ê³µ: {successful_updates}/{len(update_results)}ê°œ íšŒì‚¬")
        
        # ì„±ê³µí•œ íšŒì‚¬ë“¤
        for company_key, success in update_results.items():
            status = "âœ…" if success else "âŒ"
            logger.info(f"  {status} {company_key}")
        
        return update_results


async def fetch_edinet_data(company_code: str) -> EdinetData:
    """EDINETì—ì„œ ê¸°ì—… ë°ì´í„° ì¢…í•© ìˆ˜ì§‘"""
    edinet_data = EdinetData()
    
    async with EdinetAPI() as api:
        # 1. ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ document_idë¡œ ì²˜ë¦¬)
        zip_content = await api.get_document_package(company_code)
        if zip_content:
            # 2. honbun íŒŒì¼ë“¤ ì¶”ì¶œ
            honbun_files = api.extract_honbun_files(zip_content)
            
            if honbun_files:
                # 3. ì¸ì‚¬ ì •ë³´ íŒŒì‹±
                employee_info = api.parse_employee_info(honbun_files)
                
                # 4. EdinetHR ê°ì²´ì— ì €ì¥
                edinet_data.hr.avgTenureYears = employee_info["avgTenureYears"]
                edinet_data.hr.avgAgeYears = employee_info["avgAgeYears"]
                edinet_data.hr.avgAnnualSalaryJPY = employee_info["avgAnnualSalaryJPY"]
                
                # 5. ì¶œì²˜ ì •ë³´ ì €ì¥
                edinet_data.provenance = {
                    "source": "EDINET API v2",
                    "company_code": company_code,
                    "fetched_at": datetime.now().isoformat(),
                    "employee_info_provenance": employee_info.get("provenance", {})
                }
                
                # 6. ê¸°ë³¸ ì •ë³´ ì„¤ì • (ë¨¼ì € ê¸°ë³¸ê°’ ì„¤ì • í›„ ì§ì›ìˆ˜ ë®ì–´ì“°ê¸°)
                edinet_data.basic = parse_edinet_basic_info(company_code)
                edinet_data.basic.employee_count = employee_info["employeeCount"]
            else:
                logger.warning("honbun íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                edinet_data.basic = parse_edinet_basic_info(company_code)
        else:
            logger.warning("ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            edinet_data.basic = parse_edinet_basic_info(company_code)
    
    # ì¬ë¬´ ì •ë³´ ì„¤ì •
    edinet_data.financials = parse_edinet_financials(company_code)
    
    return edinet_data

# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
async def test_edinet_api():
    """EDINET API í…ŒìŠ¤íŠ¸"""
    company_code = "S100VZG5"  # ë¦¬ì¿ ë¥´íŠ¸ í™€ë”©ìŠ¤
    
    print("EDINET API í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    data = await fetch_edinet_data(company_code)
    
    print(f"ê¸°ì—…ëª…: {data.basic.name}")
    print(f"ì£¼ì†Œ: {data.basic.address}")
    print(f"ì§ì› ìˆ˜: {data.basic.employee_count}")
    print(f"í‰ê·  ê·¼ì†ì—°ìˆ˜: {data.hr.avgTenureYears}")
    print(f"í‰ê·  ì—°ë ¹: {data.hr.avgAgeYears}")
    print(f"í‰ê·  ì—°ë´‰: {data.hr.avgAnnualSalaryJPY}")
    print(f"ì¶œì²˜: {data.provenance}")
    
    return data

# ìµœì‹ í™” ì‹¤í–‰ í•¨ìˆ˜
async def run_edinet_update():
    """EDINET ìµœì‹ í™” ì‹¤í–‰"""
    updater = CompanyReportUpdater()
    results = await updater.run_full_update()
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "update":
        # python -m src.jap_syu.utils.edinet update
        asyncio.run(run_edinet_update())
    else:
        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_edinet_api())
