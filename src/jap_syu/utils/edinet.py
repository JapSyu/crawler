"""
EDINET API ìœ í‹¸ë¦¬í‹°
ì¼ë³¸ ê¸ˆìœµì²­ì˜ ê¸°ì—…ì •ë³´ê³µê°œì‹œìŠ¤í…œ(EDINET) APIë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì—… ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

import httpx
import asyncio
import zipfile
import io
import re
import os
import json
from typing import Dict, List, Optional, Tuple, NamedTuple
from datetime import datetime, date, timedelta
from dataclasses import dataclass

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass
from pathlib import Path
from loguru import logger
from bs4 import BeautifulSoup
from ..models import EdinetData, EdinetBasic, EdinetHR, EdinetFinancials, IRDocument

# .env íŒŒì¼ ë¡œë“œ (python-dotenvê°€ ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    # python-dotenvê°€ ì—†ìœ¼ë©´ í™˜ê²½ë³€ìˆ˜ë§Œ ì‚¬ìš©
    pass

# EDINET API ì„¤ì •
EDINET_API_BASE = "https://api.edinet-fsa.go.jp/api/v2"
EDINET_API_KEY = os.getenv("EDINET_API_KEY")

@dataclass
class CompanyDocument:
    """ë°œê²¬ëœ ê¸°ì—… ë¬¸ì„œ ì •ë³´"""
    document_id: str
    company_name: str 
    submitted_date: str
    doc_type: str
    company_key: str  # ë‚´ë¶€ ì‹ë³„ìš©
    sec_code: str = None  # ìƒì¥ë²ˆí˜¸

class EdinetAPI:
    """EDINET API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.base_url = EDINET_API_BASE
        self.session = None
        
    
    
    async def __aenter__(self):
        self.session = httpx.AsyncClient(timeout=30.0)
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.aclose()
    
    async def get_document_list(self, date: str) -> List[dict]:
        """íŠ¹ì • ë‚ ì§œì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ"""
        try:
            response = await self.session.get(
                f"{self.base_url}/documents.json",
                params={"date": date, "type": 2},  # type=2: ë©”íƒ€ë°ì´í„°ë§Œ
                headers={"Ocp-Apim-Subscription-Key": EDINET_API_KEY}
            )
            
            if response.status_code == 200:
                data = response.json()
                return data.get("results", [])
            else:
                logger.warning(f"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨ ({date}): {response.status_code}")
                
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ({date}): {e}")
        
        return []

    async def get_document_package(self, document_id: str, doc_type: str = "1") -> Optional[bytes]:
        """ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (type=1: ZIP íŒŒì¼)"""
        try:
            response = await self.session.get(
                f"{self.base_url}/documents/{document_id}",
                params={"type": doc_type},  # 1: ZIP íŒŒì¼ (iXBRL í¬í•¨)
                headers={"Ocp-Apim-Subscription-Key": EDINET_API_KEY}
            )
            
            if response.status_code == 200:
                return response.content
            else:
                logger.error(f"EDINET ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                return None
                
        except Exception as e:
            logger.error(f"EDINET ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_honbun_files(self, zip_content: bytes) -> List[Tuple[str, str]]:
        """ZIP íŒŒì¼ì—ì„œ honbun iXBRL íŒŒì¼ë“¤ ì¶”ì¶œ"""
        honbun_files = []
        
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                for file_info in zip_file.filelist:
                    filename = file_info.filename
                    if "honbun" in filename and ("ixbrl" in filename or "ix:" in filename):
                        content = zip_file.read(filename).decode('utf-8', errors='ignore')
                        honbun_files.append((filename, content))
                        logger.info(f"honbun íŒŒì¼ ë°œê²¬: {filename}")
        
        except Exception as e:
            logger.error(f"ZIP íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return honbun_files
    
    def extract_header_file(self, zip_content: bytes) -> Optional[Tuple[str, str]]:
        """ZIP íŒŒì¼ì—ì„œ header iXBRL íŒŒì¼ ì¶”ì¶œ"""
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                for file_info in zip_file.filelist:
                    filename = file_info.filename
                    if "header" in filename and ("ixbrl" in filename or filename.endswith('.htm')):
                        content = zip_file.read(filename).decode('utf-8', errors='ignore')
                        logger.info(f"header íŒŒì¼ ë°œê²¬: {filename}")
                        return (filename, content)
        
        except Exception as e:
            logger.error(f"header íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return None
    
    def parse_basic_info_from_header(self, header_content: str, filename: str) -> Dict[str, any]:
        """header íŒŒì¼ì—ì„œ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        basic_info = {
            "name_en": "",
            "headquarters": "",
            "founded_year": None,
            "sec_code": None,
        }
        
        # 1. ì˜ë¬¸ëª… ì¶”ì¶œ
        english_patterns = [
            r'CompanyNameInEnglishCoverPage">([^<]+)</ix:nonNumeric>',
            r'CompanyNameInEnglish[^>]*>([^<]+)</ix:',
            r'ã€è‹±è¨³åã€‘[^>]*>([^<]+)<',
        ]
        
        for pattern in english_patterns:
            match = re.search(pattern, header_content, re.IGNORECASE)
            if match:
                name_en = match.group(1).strip()
                if len(name_en) > 5:  # ìœ íš¨í•œ ì˜ë¬¸ëª…ì¸ ê²½ìš°
                    basic_info["name_en"] = name_en
                    logger.info(f"headerì—ì„œ ì˜ë¬¸ëª… ì¶”ì¶œ: {name_en} (íŒŒì¼: {filename})")
                    break
        
        # 2. ë³¸ì  ì£¼ì†Œ ì¶”ì¶œ
        address_patterns = [
            r'æ±äº¬éƒ½[^<\n)]+\d+ç•ª?\d*å·?',
            r'ã€’\d{3}-\d{4}[^<\n]+',
            r'æœ¬åº—[^>]*>([^<]+)',
            r'æ‰€åœ¨åœ°[^>]*>([^<]+)',
        ]
        
        for pattern in address_patterns:
            matches = re.findall(pattern, header_content, re.IGNORECASE)
            if matches:
                # ê°€ì¥ ì™„ì „í•´ ë³´ì´ëŠ” ì£¼ì†Œ ì„ íƒ (æ±äº¬éƒ½ë¡œ ì‹œì‘í•˜ê³  ë²ˆì§€ìˆ˜ê°€ ìˆëŠ” ê²ƒ) WHY? ë„ì¿„ ì•„ë‹ˆë©´ ì–´ì©”ê±´ë°
                for address in matches:
                    clean_address = address.strip()
                    if (clean_address.startswith('æ±äº¬éƒ½') and 
                        'ç•ª' in clean_address and 
                        len(clean_address) > 10):
                        basic_info["headquarters"] = clean_address
                        logger.info(f"headerì—ì„œ ë³¸ì  ì£¼ì†Œ ì¶”ì¶œ: {clean_address} (íŒŒì¼: {filename})")
                        break
                if basic_info["headquarters"]:
                    break
        
        # 3. ì„¤ë¦½ì¼ ì¶”ì¶œ (ì‹¤ì œ íšŒì‚¬ ì„¤ë¦½ì¼, ë³´ê³ ì„œ ë‚ ì§œê°€ ì•„ë‹Œ)ã€€ì´ê±´ ë³¸ë¬¸ì—ì„œ ì°¾ì•„ì•¼í• ë“¯. í—¤ë”ì—” ì•ˆë‚˜ì™€ìˆìŒ
        # í—¤ë”ì—ì„œ ì„¤ë¦½ë…„ë„ ì¶”ì¶œ ë¹„í™œì„±í™” - ì‚¬ì—…ë…„ë„ ê³„ì‚° ìš°ì„  ì‚¬ìš©
        # founded_patterns = [
        #     r'è¨­ç«‹[^>]*>([^<]*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[^<]*)',
        #     r'è¨­ç«‹å¹´æœˆæ—¥[^>]*>([^<]*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[^<]*)',
        #     r'å‰µç«‹[^>]*>([^<]*(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥[^<]*)',
        # ]
        # 
        # for pattern in founded_patterns:
        #     matches = re.findall(pattern, header_content)
        #     if matches:
        #         for match in matches:
        #             year = int(match[1]) if len(match) > 1 else None
        #             # 1800ë…„ëŒ€~2000ë…„ëŒ€ ì´ˆë°˜ì˜ í•©ë¦¬ì ì¸ ì„¤ë¦½ë…„ë„ë§Œ í—ˆìš©
        #             if year and 1800 <= year <= 2020:
        #                 basic_info["founded_year"] = year
        #                 logger.info(f"headerì—ì„œ ì„¤ë¦½ë…„ë„ ì¶”ì¶œ: {year} (íŒŒì¼: {filename})")
        #                 break
        #         if basic_info["founded_year"]:
        #             break
        logger.info(f"í—¤ë”ì—ì„œ ì„¤ë¦½ë…„ë„ ì¶”ì¶œ ê±´ë„ˆëœ€ - ì‚¬ì—…ë…„ë„ ê³„ì‚° ìš°ì„  ì‚¬ìš© (íŒŒì¼: {filename})")
        
        # 4. ìƒì¥ë²ˆí˜¸ ì¶”ì¶œ (iXBRL íƒœê·¸ì—ì„œ)
        sec_code_patterns = [
            r'SecurityCodeDEI">(\d{4})</ix:nonNumeric>',
            r'securityCode[^0-9]*(\d{4})',
            r'è¯åˆ¸[ã‚³]?[ãƒ¼]?[ãƒ‰][^0-9]*(\d{4})',
            r'[è¨¼åˆ¸]?[ã‚³]?[ãƒ¼]?[ãƒ‰]\s*[ï¼š:]\s*(\d{4})',
        ]
        
        for pattern in sec_code_patterns:
            match = re.search(pattern, header_content, re.IGNORECASE)
            if match:
                sec_code = match.group(1)
                if len(sec_code) == 4 and sec_code.isdigit():
                    basic_info["sec_code"] = sec_code
                    logger.info(f"headerì—ì„œ ìƒì¥ë²ˆí˜¸ ì¶”ì¶œ: {sec_code} (íŒŒì¼: {filename})")
                    break
        
        return basic_info
    
    def parse_employee_info(self, honbun_files: List[Tuple[str, str]]) -> Dict[str, any]:
        """honbun íŒŒì¼ë“¤ì—ì„œ ì¸ì‚¬ ì •ë³´ ì¶”ì¶œ"""
        employee_info = {
            "avgTenureYears": None,
            "avgAgeYears": None,
            "avgAnnualSalaryJPY": None,
            "employeeCount": None,
            "provenance": {}
        }
        
        # ì°¾ì„ í‚¤ì›Œë“œë“¤
        keywords = {
            "avgTenureYears": ["å¹³å‡å‹¤ç¶šå¹´æ•°", "AverageLengthOfServiceYears"],
            "avgAgeYears": ["å¹³å‡å¹´é½¢", "AverageAgeYears"],
            "avgAnnualSalaryJPY": ["å¹³å‡å¹´é–“çµ¦ä¸", "AverageAnnualSalary"],
            "employeeCount": ["å¾“æ¥­å“¡æ•°", "NumberOfEmployees"]
        }
        
        # iXBRL ê°œë… ì´ë¦„ë“¤ (ì‹¤ì œ name ì†ì„±ê°’)
        ixbrl_concepts = {
            "avgTenureYears": [
                "jpcrp_cor:AverageLengthOfServiceYearsInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageLengthOfServiceYearsInformationOfReportingCompanyInformation",
                "jpcrp_cor:AverageLengthOfServiceYears"
            ],
            "avgAgeYears": [
                "jpcrp_cor:AverageAgeYearsInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageAgeYearsInformationOfReportingCompanyInformation", 
                "jpcrp_cor:AverageAgeYears",
                "jpcrp_cor:AverageAge"
            ],
            "avgAnnualSalaryJPY": [
                "jpcrp_cor:AverageAnnualSalaryInformationAboutReportingCompanyInformationAboutEmployees",
                "jpcrp_cor:AverageAnnualSalaryInformationOfReportingCompanyInformation",
                "jpcrp_cor:AverageAnnualSalary"
            ],
            "employeeCount": [
                "jpcrp_cor:NumberOfEmployees",
                "jpcrp_cor:AverageNumberOfEmployees"
            ]
        }
        
        for filename, content in honbun_files:
            logger.info(f"ì§ì› ì •ë³´ ì¶”ì¶œ ì‹œë„: {filename}")
            
            # ê° ì§€í‘œë³„ë¡œ ì¶”ì¶œ
            for field in keywords.keys():
                if employee_info[field] is not None:
                    continue  # ì´ë¯¸ ì°¾ì•˜ìœ¼ë©´ ìŠ¤í‚µ
                
                # 1. ìš°ì„  iXBRL ê°œë…ìœ¼ë¡œ ê²€ìƒ‰ (ê°€ì¥ ì •í™•)
                if field in ixbrl_concepts:
                    for concept_name in ixbrl_concepts[field]:
                        results = self._extract_value_from_ixbrl_concept(content, concept_name, filename)
                        if results:
                            # contextRefë¡œ ìµœì  ê°’ ì„ íƒ
                            best_result = self._select_best_value_by_context(results)
                            # scale/decimals/unitRef ì†ì„± ì ìš©í•˜ì—¬ ìµœì¢… ê°’ ê³„ì‚°
                            final_value = self._apply_ixbrl_attributes(best_result, field)
                            employee_info[field] = final_value
                            employee_info["provenance"][field] = {
                                "file": filename,
                                "concept": concept_name,
                                "contextRef": best_result['contextRef'],
                                "unitRef": best_result['unitRef'],
                                "scale": best_result['scale'],
                                "method": "ixbrl_concept"
                            }
                            logger.info(f"{field}: iXBRL ê°œë…ìœ¼ë¡œ ê°’ ì¶”ì¶œ ì„±ê³µ - {best_result['value']}")
                            break
                
                # 2. iXBRLë¡œ ëª» ì°¾ìœ¼ë©´ í‚¤ì›Œë“œ ë°±ì—…
                if employee_info[field] is None:
                    for keyword in keywords[field]:
                        if keyword in content:
                            value = self._extract_value_from_context(content, keyword, filename)
                            if value is not None:
                                employee_info[field] = value
                                employee_info["provenance"][field] = {
                                    "file": filename,
                                    "keyword": keyword,
                                    "method": "text_search_backup"
                                }
                                logger.info(f"{field}: í…ìŠ¤íŠ¸ ë°±ì—…ìœ¼ë¡œ ê°’ ì¶”ì¶œ - {value}")
                                break
        
        return employee_info
    
    def parse_basic_info(self, honbun_files: List[Tuple[str, str]], zip_content: bytes = None) -> Dict[str, any]:
        """honbun íŒŒì¼ë“¤ê³¼ header íŒŒì¼ì—ì„œ ê¸°ì—… ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        logger.info(f"ğŸ” parse_basic_info í˜¸ì¶œë¨ - {len(honbun_files)}ê°œ honbun íŒŒì¼ ì²˜ë¦¬")
        basic_info = {
            "name": "",
            "name_en": "",
            "headquarters": "",
            "founded_year": None,
            "sec_code": None,
            "provenance": {}
        }
        
        # í—¤ë”ì—ì„œ ì¶”ì¶œí•œ ì„¤ë¦½ë…„ë„ ì„ì‹œ ì €ì¥ (ì—°í˜ ìš°ì„ , ì—†ìœ¼ë©´ ì‚¬ìš©)
        header_founded_year = None
        header_filename_for_founded = None
        
        # ë¨¼ì € header íŒŒì¼ì—ì„œ ì •ë³´ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„)
        if zip_content:
            header_file = self.extract_header_file(zip_content)
            if header_file:
                filename, content = header_file
                header_info = self.parse_basic_info_from_header(content, filename)
                
                # headerì—ì„œ ì¶”ì¶œí•œ ì •ë³´ë¡œ ìš°ì„  ì±„ì›€
                if header_info["name_en"]:
                    basic_info["name_en"] = header_info["name_en"]
                    basic_info["provenance"]["name_en"] = {"file": filename, "method": "header_ixbrl"}
                
                if header_info["headquarters"]:
                    basic_info["headquarters"] = header_info["headquarters"]
                    basic_info["provenance"]["headquarters"] = {"file": filename, "method": "header_ixbrl"}
                
                # ì‚¬ì—…ë…„ë„ì—ì„œ ì„¤ë¦½ë…„ë„ ê³„ì‚° (headerì—ì„œ ìš°ì„  ì‹œë„)
                header_founded_year = self._extract_founded_year(content, filename, zip_content)
                if header_founded_year:
                    basic_info["founded_year"] = header_founded_year
                    basic_info["provenance"]["founded_year"] = {"file": filename, "method": "header_business_year"}
                    logger.info(f"í—¤ë”ì—ì„œ ì„¤ë¦½ë…„ë„ ê³„ì‚° ì™„ë£Œ: {header_founded_year}ë…„ (íŒŒì¼: {filename})")
                
                if header_info["sec_code"]:
                    basic_info["sec_code"] = header_info["sec_code"]
                    basic_info["provenance"]["sec_code"] = {"file": filename, "method": "header_ixbrl"}
        
        for filename, content in honbun_files:
            # 1. íšŒì‚¬ëª… ì¶”ì¶œ (æå‡ºä¼šç¤¾ã®çŠ¶æ³)
            if not basic_info["name"]:
                name = self._extract_company_name(content, filename)
                if name:
                    basic_info["name"] = name
                    basic_info["provenance"]["name"] = {"file": filename, "method": "regex"}
            
            # 2. ì˜ë¬¸ëª… ì¶”ì¶œ (è‹±è¨³å)
            if not basic_info["name_en"]:
                name_en = self._extract_company_name_en(content, filename)
                if name_en:
                    basic_info["name_en"] = name_en
                    basic_info["provenance"]["name_en"] = {"file": filename, "method": "regex"}
            
            # 3. ë³¸ì  ì†Œì¬ì§€ ì¶”ì¶œ (æœ¬åº—ã®æ‰€åœ¨ã®å ´æ‰€)
            if not basic_info["headquarters"]:
                headquarters = self._extract_headquarters(content, filename)
                if headquarters:
                    basic_info["headquarters"] = headquarters
                    basic_info["provenance"]["headquarters"] = {"file": filename, "method": "regex"}
            
            # 4. ì„¤ë¦½ë…„ë„ëŠ” ì´ë¯¸ headerì—ì„œ ì²˜ë¦¬ë¨ (ì—°í˜ ì¶”ì¶œì€ ì„±ëŠ¥ìƒ ìŠ¤í‚µ)
            # í—¤ë”ì˜ ì‚¬ì—…ë…„ë„ ì •ë³´ê°€ ë” ì •í™•í•˜ë¯€ë¡œ ìš°ì„  ì‚¬ìš©
            
            # 5. ìƒì¥ë²ˆí˜¸ ì¶”ì¶œ (ì¦åˆ¸ã‚³ãƒ¼ãƒ‰)
            if not basic_info["sec_code"]:
                sec_code = self._extract_security_code(content, filename)
                if sec_code:
                    basic_info["sec_code"] = sec_code
                    basic_info["provenance"]["sec_code"] = {"file": filename, "method": "regex"}
        
        # ì„¤ë¦½ë…„ë„ëŠ” ì´ë¯¸ headerì—ì„œ ì²˜ë¦¬ë¨
        
        return basic_info
    
    def _extract_company_name(self, content: str, filename: str) -> Optional[str]:
        """íšŒì‚¬ëª… ì¶”ì¶œ"""
        patterns = [
            r"ä¼šç¤¾å[^a-zA-Zï½-ï½šï¼-ï¼™]*?([^\s\n]+æ ªå¼ä¼šç¤¾[^\s\n]*)",
            r"å•†å·[^a-zA-Zï½-ï½šï¼-ï¼™]*?([^\s\n]+æ ªå¼ä¼šç¤¾[^\s\n]*)",
            r"æå‡ºä¼šç¤¾ã®çŠ¶æ³[^a-zA-Z]*?ä¼šç¤¾å[^a-zA-Z]*?([^\s\n]+æ ªå¼ä¼šç¤¾[^\s\n]*)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                name = match.group(1).strip()
                
                # HTML íƒœê·¸ ì œê±°
                name = re.sub(r'<[^>]+>', '', name)
                # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ã€Œã€, (), ã€, ã€‚, ë“±)
                name = re.sub(r'[ã€Œã€ï¼ˆï¼‰()ã€ã€‚]', '', name)
                name = re.sub(r'\s+', ' ', name).strip()
                name = name.rstrip('.,').strip()
                
                # ìœ íš¨ì„± ê²€ì‚¬
                if name and len(name) > 3 and not re.search(r'[<>]', name):
                    logger.info(f"íšŒì‚¬ëª… ì¶”ì¶œ: {name} (íŒŒì¼: {filename})")
                    return name
        return None
    
    def _extract_company_name_en(self, content: str, filename: str) -> Optional[str]:
        """ì˜ë¬¸ëª… ì¶”ì¶œ (fallback) - í—¤ë”ì—ì„œ ì°¾ì§€ ëª»í•œ ê²½ìš°ì˜ ë°±ì—…"""
        
        # ì¼ë°˜ì ì¸ ì˜ë¬¸ íšŒì‚¬ëª… íŒ¨í„´ë“¤ (ë³¸ë¬¸ì—ì„œ Inc, Corp, Company ë“± ì°¾ê¸°)
        
        patterns = [
            r'([A-Z][A-Za-z\s]*Holdings[A-Za-z\s]*Limited)',
            r'([A-Z][A-Za-z\s]*Corporation)',
            r'([A-Z][A-Za-z\s]*Holdings)',
            r'([A-Z][A-Za-z\s]*Group)',
            r'([A-Z][A-Za-z\s]*Inc\.?)',
            r'([A-Z][A-Za-z\s]*Corp\.?)', 
            r'([A-Z][A-Za-z\s]*Company)',
            r'([A-Z][A-Za-z\s]*Co\.?)',
            r'([A-Z][A-Za-z\s]*Ltd\.?)',
            r'([A-Z][A-Za-z\s]*Limited)',
        ]
        
        for i, pattern in enumerate(patterns):
            matches = re.finditer(pattern, content, re.IGNORECASE)
            for match in matches:
                name_en = match.group(1).strip()
                
                # HTML íƒœê·¸ ì œê±°
                name_en = re.sub(r'<[^>]+>', '', name_en)
                # ë¶ˆí•„ìš”í•œ ë¬¸ìë“¤ ì œê±°
                name_en = re.sub(r'[ã€ã€‚ï¼‰\)]', '', name_en)
                name_en = re.sub(r'\s+', ' ', name_en).strip()
                name_en = name_en.rstrip('.,').strip()
                
                # ìœ íš¨ì„± ì²´í¬
                if (len(name_en) >= 8 and 
                    re.search(r'[A-Z]', name_en) and  # ëŒ€ë¬¸ì í¬í•¨
                    ('Holdings' in name_en or 'Limited' in name_en or 'Inc' in name_en or 'Corp' in name_en or 'Company' in name_en)):
                    logger.info(f"ì˜ë¬¸ëª… ì¶”ì¶œ ì„±ê³µ (íŒ¨í„´ {i+1}): {name_en} (íŒŒì¼: {filename})")
                    return name_en
                elif len(name_en) >= 5:
                    logger.debug(f"ì˜ë¬¸ëª… í›„ë³´ (íŒ¨í„´ {i+1}): '{name_en}' - ì¡°ê±´ ë¯¸ë‹¬")
        
        logger.warning(f"ì˜ë¬¸ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (íŒŒì¼: {filename})")
        return None
    
    def _extract_headquarters(self, content: str, filename: str) -> Optional[str]:
        """ë³¸ì  ì†Œì¬ì§€ ì¶”ì¶œ (æœ¬åº—ã®æ‰€åœ¨ã®å ´æ‰€)"""
        patterns = [
            # ì¼ë³¸ ì£¼ìš” ë„ë„ë¶€í˜„ ì£¼ì†Œ íŒ¨í„´ë“¤ (êµ¬ì²´ì ì¸ ì£¼ì†Œ)
            r"(æ±äº¬éƒ½[^<\n)]+\d+ç•ª?\d*å·?)",
            r"(å¤§é˜ª[åºœå¸‚][^<\n)]+\d+ç•ª?\d*å·?)",
            r"(äº¬éƒ½[åºœå¸‚][^<\n)]+\d+ç•ª?\d*å·?)",
            r"(ç¥å¥ˆå·çœŒ[^<\n)]+\d+ç•ª?\d*å·?)",
            r"(æ„›çŸ¥çœŒ[^<\n)]+\d+ç•ª?\d*å·?)",
            r"(ç¦å²¡çœŒ[^<\n)]+\d+ç•ª?\d*å·?)",
            r"(åŒ—æµ·é“[^<\n)]+\d+ç•ª?\d*å·?)",
            r"([^<\n)]*[éƒ½é“åºœçœŒå¸‚åŒºç”ºæ‘][^<\n)]+\d+ç•ª?\d*å·?)",  # ì¼ë°˜ì ì¸ íŒ¨í„´
            
            # ìš°í¸ë²ˆí˜¸ê°€ ìˆëŠ” ì£¼ì†Œ
            r"(ã€’\d{3}-\d{4}[^<\n]+)",
            
            # ì „í†µì ì¸ ë³¸ì  íŒ¨í„´ë“¤
            r"æœ¬åº—ã®æ‰€åœ¨ã®å ´æ‰€[^>]*>([^<]+)",
            r"æœ¬åº—ã®æ‰€åœ¨ã®å ´æ‰€[^\n]*?([^\n<]+)",
            r"æœ¬åº—æ‰€åœ¨åœ°[^>]*>([^<]+)",
            r"æœ¬åº—æ‰€åœ¨åœ°[^\n]*?([^\n<]+)",
            r"æœ¬ç¤¾æ‰€åœ¨åœ°[^>]*>([^<]+)",
            r"æœ¬ç¤¾æ‰€åœ¨åœ°[^\n]*?([^\n<]+)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                # ê·¸ë£¹ì´ ìˆëŠ” íŒ¨í„´ì¸ì§€ í™•ì¸ (ê´„í˜¸ê°€ í¬í•¨ëœ íŒ¨í„´)
                if match.groups():
                    headquarters = match.group(1).strip()
                else:
                    headquarters = match.group(0).strip()
                    
                # HTML íƒœê·¸ ì œê±°
                headquarters = re.sub(r'<[^>]+>', '', headquarters)
                # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
                headquarters = re.sub(r'^[:\sãƒ»]*', '', headquarters)
                headquarters = re.sub(r'[>\]]*$', '', headquarters)  # ëì˜ > ë¬¸ì ì œê±°
                headquarters = headquarters.strip()
                
                # ìœ íš¨ì„± ê²€ì‚¬ (HTML íƒœê·¸ë‚˜ ì´ìƒí•œ ë¬¸ìë“¤ í•„í„°ë§)
                if (headquarters and len(headquarters) > 3 and 
                    not re.search(r'[<>]', headquarters) and
                    not headquarters.startswith('ã‚’') and
                    not headquarters.endswith('</b></p>')):
                    logger.info(f"ë³¸ì  ì†Œì¬ì§€ ì¶”ì¶œ: {headquarters} (íŒŒì¼: {filename})")
                    return headquarters
        return None
    
    def _extract_submission_year(self, content: str, filename: str) -> Optional[int]:
        """ì œì¶œì¼(æå‡ºæ—¥)ì—ì„œ ë…„ë„ ì¶”ì¶œ"""
        try:
            # ì œì¶œì¼ íŒ¨í„´ë“¤ (ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›)
            submission_patterns = [
                r"æå‡ºæ—¥[^0-9]*?(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",  # æå‡ºæ—¥: 2025å¹´6æœˆ23æ—¥
                r"æå‡ºæ—¥[^0-9]*?(\d{4})å¹´",  # æå‡ºæ—¥: 2025å¹´
                r"æå‡ºå¹´æœˆæ—¥[^0-9]*?(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥",  # æå‡ºå¹´æœˆæ—¥: 2025å¹´6æœˆ23æ—¥
                r"æå‡ºå¹´æœˆæ—¥[^0-9]*?(\d{4})å¹´",  # æå‡ºå¹´æœˆæ—¥: 2025å¹´
                r"SubmissionDate[^0-9]*?(\d{4})",  # SubmissionDate: 2025
                r"DocumentPeriodEndDate[^0-9]*?(\d{4})",  # DocumentPeriodEndDate: 2025
                # iXBRL íƒœê·¸ì—ì„œ ì¶”ì¶œ
                r'<ix:nonNumeric[^>]*name="[^"]*SubmissionDate[^"]*"[^>]*>.*?(\d{4})å¹´.*?</ix:nonNumeric>',
                r'<ix:nonNumeric[^>]*name="[^"]*DocumentPeriodEndDate[^"]*"[^>]*>.*?(\d{4})å¹´.*?</ix:nonNumeric>',
                # íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ (ì˜ˆ: 2025-06-23)
                r'(\d{4})-\d{2}-\d{2}',
            ]
            
            for pattern in submission_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
                if matches:
                    # ì²« ë²ˆì§¸ ë§¤ì¹˜ì—ì„œ ë…„ë„ ì¶”ì¶œ
                    year_match = matches[0]
                    if isinstance(year_match, tuple):
                        year = int(year_match[0])  # ì²« ë²ˆì§¸ ê·¸ë£¹ì´ ë…„ë„
                    else:
                        year = int(year_match)
                    
                    # í•©ë¦¬ì ì¸ ë…„ë„ ë²”ìœ„ ì²´í¬
                    if 2020 <= year <= 2030:
                        logger.info(f"ì œì¶œì¼ì—ì„œ ë…„ë„ ì¶”ì¶œ: {year}ë…„ (íŒ¨í„´: {pattern}, íŒŒì¼: {filename})")
                        return year
                    else:
                        logger.debug(f"ì œì¶œì¼ ë…„ë„ ë²”ìœ„ ë²—ì–´ë‚¨: {year}ë…„ (íŒŒì¼: {filename})")
            
            # íŒŒì¼ëª…ì—ì„œ ì œì¶œì¼ ë…„ë„ ì¶”ì¶œ ì‹œë„ (ë§ˆì§€ë§‰ ë‚ ì§œê°€ ì œì¶œì¼)
            # ì˜ˆ: 2025-03-31_01_2025-06-20 â†’ 2025-06-20ì´ ì œì¶œì¼
            filename_date_matches = re.findall(r'(\d{4})-(\d{2})-(\d{2})', filename)
            if filename_date_matches:
                # ë§ˆì§€ë§‰ ë‚ ì§œë¥¼ ì œì¶œì¼ë¡œ ê°„ì£¼
                submission_date = filename_date_matches[-1]
                year = int(submission_date[0])
                if 2020 <= year <= 2030:
                    logger.info(f"íŒŒì¼ëª…ì—ì„œ ì œì¶œì¼ ë…„ë„ ì¶”ì¶œ: {year}ë…„ (íŒŒì¼: {filename})")
                    return year
            
            logger.warning(f"ì œì¶œì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ (íŒŒì¼: {filename})")
            return None
            
        except Exception as e:
            logger.error(f"ì œì¶œì¼ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e} (íŒŒì¼: {filename})")
            return None
    
    def _extract_founded_year(self, content: str, filename: str, zip_content: bytes = None) -> Optional[int]:
        """ì„¤ë¦½ë…„ë„ ì¶”ì¶œ (äº‹æ¥­å¹´åº¦ ì£¼ê¸°ì—ì„œ ê³„ì‚°)"""
        try:
            # 1. ë¨¼ì € ì œì¶œì¼(æå‡ºæ—¥) ì¶”ì¶œ
            submission_year = self._extract_submission_year(content, filename)
            if not submission_year:
                logger.warning(f"ì œì¶œì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì„¤ë¦½ë…„ë„ ê³„ì‚° ë¶ˆê°€ (íŒŒì¼: {filename})")
                return None
            
            logger.info(f"ğŸ“… ì œì¶œì¼ ê¸°ì¤€ë…„ë„: {submission_year}ë…„")
            
            # 2. äº‹æ¥­å¹´åº¦(ì‚¬ì—…ë…„ë„) ì£¼ê¸° ì •ë³´ì—ì„œ ê³„ì‚° - HTML íƒœê·¸ ê³ ë ¤
            business_year_patterns = [
                r"ç¬¬(\d+)æœŸ[^<]*?äº‹æ¥­å¹´åº¦",  # ç¬¬65æœŸ (ì 2024ë…„...è‡³ 2025ë…„...äº‹æ¥­å¹´åº¦)
                r"ç¬¬(\d+)æœŸ[^<]*?\(",  # ç¬¬65æœŸ (ì 2024ë…„...
                r"ç¬¬(\d+)æœŸ",  # ç¬¬65æœŸ ë‹¨ë…
                r"(\d+)æœŸ[^<]*?äº‹æ¥­å¹´åº¦",  # 65æœŸ...äº‹æ¥­å¹´åº¦
                r"äº‹æ¥­å¹´åº¦[^<]*?ç¬¬(\d+)æœŸ"  # äº‹æ¥­å¹´åº¦...ç¬¬65æœŸ
            ]
            
            # HTML íƒœê·¸ ë‚´ë¶€ë„ ê²€ìƒ‰ (ì—…ë°ì´íŠ¸: ë…„ë„ë„ í•¨ê»˜ ì¶”ì¶œ)
            html_patterns = [
                r'<ix:nonNumeric[^>]*>.*?ç¬¬(\d+)æœŸ.*?(\d{4})å¹´.*?</ix:nonNumeric>',  # HTML íƒœê·¸ ë‚´ë¶€ì—ì„œ ê¸°ìˆ˜ì™€ ë…„ë„ í•¨ê»˜
                r'<ix:nonNumeric[^>]*>.*?ç¬¬(\d+)æœŸ.*?</ix:nonNumeric>',  # HTML íƒœê·¸ ë‚´ë¶€
                r'>ç¬¬(\d+)æœŸ[^<]*?äº‹æ¥­å¹´åº¦<',  # íƒœê·¸ ì‚¬ì´
                r'>ç¬¬(\d+)æœŸ[^<]*?\(<',  # íƒœê·¸ ì‚¬ì´
            ]
            
            # ë””ë²„ê¹…: ì‹¤ì œ í…ìŠ¤íŠ¸ ì¼ë¶€ í™•ì¸
            header_text = content[:3000]
            logger.info(f"ğŸ” ì„¤ë¦½ë…„ë„ ê³„ì‚° ë””ë²„ê¹… - ë¬¸ì„œ í—¤ë” (ì²˜ìŒ 1000ì): {header_text[:1000]}")
            
            all_patterns = business_year_patterns + html_patterns
            
            for pattern in all_patterns:
                matches = re.findall(pattern, content[:15000], re.DOTALL)  # ë” ë„“ì€ ë²”ìœ„ì—ì„œ ê²€ìƒ‰
                if matches:
                    logger.info(f"ğŸ“Š ì‚¬ì—…ë…„ë„ íŒ¨í„´ ë§¤ì¹­: {pattern} â†’ {matches}")
                    
                    # ë§¤ì¹˜ ê²°ê³¼ ë¶„ì„
                    if isinstance(matches[0], tuple) and len(matches[0]) >= 2:
                        # íŠœí”Œì¸ ê²½ìš°: (ê¸°ìˆ˜, ë…„ë„) í˜•íƒœ
                        period = int(matches[0][0])
                        # ì œì¶œì¼ ê¸°ì¤€ë…„ë„ ì‚¬ìš©
                        current_year = submission_year
                        logger.info(f"ğŸ“Š íŒ¨í„´ì—ì„œ ê¸°ìˆ˜ ì¶”ì¶œ: ì œ{period}ê¸°, ì œì¶œì¼ ê¸°ì¤€ë…„ë„: {current_year}ë…„")
                    else:
                        # ë‹¨ì¼ ê°’ì¸ ê²½ìš°: ê¸°ìˆ˜ë§Œ ì¶”ì¶œ
                        period = int(matches[0]) if isinstance(matches[0], str) else int(matches[0][0] if isinstance(matches[0], tuple) else matches[0])
                        # ì œì¶œì¼ ê¸°ì¤€ë…„ë„ ì‚¬ìš©
                        current_year = submission_year
                        logger.info(f"ğŸ“Š íŒ¨í„´ì—ì„œ ê¸°ìˆ˜ ì¶”ì¶œ: ì œ{period}ê¸°, ì œì¶œì¼ ê¸°ì¤€ë…„ë„: {current_year}ë…„")
                    
                    if current_year:
                        founded_year = current_year - period + 1  # +1ì€ ì°½ë¦½ë…„ë„ ë³´ì •
                        logger.info(f"ğŸ§® ê³„ì‚°: {current_year}ë…„ - {period}ê¸° + 1 = {founded_year}ë…„")
                        
                        if 1850 <= founded_year <= current_year:
                            logger.info(f"äº‹æ¥­å¹´åº¦ì—ì„œ ì„¤ë¦½ë…„ë„ ê³„ì‚°: ì œ{period}ê¸° â†’ {founded_year}ë…„ (íŒŒì¼: {filename})")
                            return founded_year
                        else:
                            logger.warning(f"ì„¤ë¦½ë…„ë„ ë²”ìœ„ ë²—ì–´ë‚¨: {founded_year}ë…„ (1850-{current_year} ë²”ìœ„)")
                    else:
                        logger.warning(f"ê¸°ì¤€ë…„ë„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (íŒ¨í„´ ë§¤ì¹­: {pattern})")
                else:
                    logger.debug(f"âŒ íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨: {pattern}")
            
            # 3. ì‚¬ì—…ë…„ë„ ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ honbun íŒŒì¼ë“¤ì—ì„œ ì„¤ë¦½ë…„ë„ ì§ì ‘ ê²€ìƒ‰
            logger.info(f"honbun íŒŒì¼ë“¤ì—ì„œ ì„¤ë¦½ë…„ë„ ì§ì ‘ ê²€ìƒ‰ ì‹œë„...")
            
            # honbun íŒŒì¼ë“¤ì—ì„œ ì„¤ë¦½ë…„ë„ íŒ¨í„´ ê²€ìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
            founded_patterns = [
                # ê°€ì¥ ì •í™•í•œ íŒ¨í„´ë“¤ (1900ë…„ëŒ€ ì„¤ë¦½ë…„ë„ ìš°ì„ )
                r"(19\d{2})å¹´.*?è¨­ç«‹",
                r"(19\d{2})å¹´.*?å‰µç«‹", 
                r"(19\d{2})å¹´.*?å‰µæ¥­",
                # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ (ë…„ë„ê°€ ì•ì— ì˜¤ëŠ” ê²½ìš°)
                r"(\d{4})å¹´.*?è¨­ç«‹",
                r"(\d{4})å¹´.*?å‰µç«‹", 
                r"(\d{4})å¹´.*?å‰µæ¥­",
                # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤ (ì„¤ë¦½ì´ ì•ì— ì˜¤ëŠ” ê²½ìš°)
                r"è¨­ç«‹.*?(\d{4})å¹´",
                r"å‰µç«‹.*?(\d{4})å¹´", 
                r"è¨­ç«‹å¹´.*?(\d{4})",
                r"Founded.*?(\d{4})",
                r"Established.*?(\d{4})",
                # íŠ¹ìˆ˜ íŒ¨í„´ë“¤
                r"ä¼šç¤¾è¨­ç«‹.*?(\d{4})å¹´",
                r"æ³•äººè¨­ç«‹.*?(\d{4})å¹´",
                r"è¨­ç«‹ç™»è¨˜.*?(\d{4})å¹´",
                r"å‰µæ¥­.*?(\d{4})å¹´"
            ]
            
            # honbun íŒŒì¼ë“¤ì—ì„œ ê²€ìƒ‰ (header íŒŒì¼ + honbun íŒŒì¼ë“¤)
            search_contents = [(filename, content)]
            
            # honbun íŒŒì¼ë“¤ë„ ì¶”ê°€
            honbun_files = self.extract_honbun_files(zip_content) if hasattr(self, 'extract_honbun_files') else []
            for honbun_filename, honbun_content in honbun_files[:5]:  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ ê²€ìƒ‰
                search_contents.append((honbun_filename, honbun_content))
            
            for pattern in founded_patterns:
                for file_name, file_content in search_contents:
                    matches = re.findall(pattern, file_content, re.IGNORECASE)
                    if matches:
                        logger.info(f"ì„¤ë¦½ë…„ë„ íŒ¨í„´ ë§¤ì¹˜: {pattern} â†’ {matches[:3]} (íŒŒì¼: {file_name})")
                        
                        # ê°€ì¥ ì˜¤ë˜ëœ ë…„ë„ë¥¼ ì„¤ë¦½ë…„ë„ë¡œ ì„ íƒ
                        valid_years = []
                        for match in matches:
                            year = int(match) if isinstance(match, str) else int(match[0])
                            if 1850 <= year <= submission_year:
                                valid_years.append(year)
                        
                        if valid_years:
                            founded_year = min(valid_years)  # ê°€ì¥ ì˜¤ë˜ëœ ë…„ë„
                            logger.info(f"honbunì—ì„œ ì„¤ë¦½ë…„ë„ ì¶”ì¶œ: {founded_year}ë…„ (íŒ¨í„´: {pattern}, íŒŒì¼: {file_name})")
                            return founded_year
            
            logger.info(f"ì„¤ë¦½ë…„ë„ ì¶”ì¶œ ì‹¤íŒ¨ - äº‹æ¥­å¹´åº¦ì™€ í—¤ë” ëª¨ë‘ì—ì„œ ë°œê²¬ ì•ˆë¨ (íŒŒì¼: {filename})")
            return None
            
        except Exception as e:
            logger.warning(f"ì„¤ë¦½ë…„ë„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e} (íŒŒì¼: {filename})")
            return None
    
    def _extract_security_code(self, content: str, filename: str) -> Optional[str]:
        """ìƒì¥ë²ˆí˜¸ ì¶”ì¶œ (è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰)"""
        patterns = [
            r"è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰[^\d]*?(\d{4})",
            r"éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰[^\d]*?(\d{4})",
            r"ã‚³ãƒ¼ãƒ‰ç•ªå·[^\d]*?(\d{4})"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, content)
            if match:
                sec_code = match.group(1)
                logger.info(f"ìƒì¥ë²ˆí˜¸ ì¶”ì¶œ: {sec_code} (íŒŒì¼: {filename})")
                return sec_code
        return None
    
    async def _translate_to_korean(self, japanese_text: str) -> str:
        """ì¼ë³¸ì–´ í…ìŠ¤íŠ¸ë¥¼ í•œê¸€ë¡œ ë²ˆì—­ (Google Translate API ì‚¬ìš©)"""
        if not japanese_text:
            return ""
        
        # ë¨¼ì € í•˜ë“œì½”ë”©ëœ ë§¤í•‘ í…Œì´ë¸”ì—ì„œ í™•ì¸ (ë¹ ë¥¸ ì²˜ë¦¬)
        translation_map = {
            # íšŒì‚¬ íƒ€ì…
            "æ ªå¼ä¼šç¤¾": "",
            "ãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹": "í™€ë”©ìŠ¤",
            "ã‚°ãƒ«ãƒ¼ãƒ—": "ê·¸ë£¹",
            "ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³": "ì½”í¼ë ˆì´ì…˜",
            "ã‚¤ãƒ³ã‚¯": "",
            
            # ì£¼ìš” íšŒì‚¬ëª…ë“¤
            "ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯": "ì†Œí”„íŠ¸ë±…í¬",
            "ãƒªã‚¯ãƒ«ãƒ¼ãƒˆ": "ë¦¬ì¿ ë¥´íŠ¸",
            "ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ": "ì‚¬ì´ë²„ì—ì´ì „íŠ¸",
            "ãƒ¡ãƒ«ã‚«ãƒª": "ë©”ë¥´ì¹´ë¦¬",
            "æ¥½å¤©": "ë¼ì¿ í…",
            "ãƒ‡ã‚£ãƒ¼ãƒ»ã‚¨ãƒŒãƒ»ã‚¨ãƒ¼": "DeNA",
            "ã‚½ãƒ‹ãƒ¼": "ì†Œë‹ˆ",
            "å¯Œå£«é€š": "í›„ì§€ì¯”",
            "ã‚¨ãƒŒãƒ»ãƒ†ã‚£ãƒ»ãƒ†ã‚£ãƒ»ãƒ‡ãƒ¼ã‚¿": "NTTë°ì´í„°",
            "ï¼¬ï¼©ï¼®ï¼¥ãƒ¤ãƒ•ãƒ¼": "ë¼ì¸ì•¼í›„",
            
            # ê¸°ìˆ /ì‚°ì—… ìš©ì–´ë“¤
            "ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼": "í…Œí¬ë†€ë¡œì§€",
            "ã‚·ã‚¹ãƒ†ãƒ ": "ì‹œìŠ¤í…œ",
            "ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢": "ì†Œí”„íŠ¸ì›¨ì–´",
            "ãƒ‡ã‚¸ã‚¿ãƒ«": "ë””ì§€í„¸",
            "ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆ": "ì¸í„°ë„·",
            "ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼": "ì»´í“¨í„°",
            "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯": "ë„¤íŠ¸ì›Œí¬",
            "ã‚µãƒ¼ãƒ“ã‚¹": "ì„œë¹„ìŠ¤",
            "ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³": "ì†”ë£¨ì…˜",
            "ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³": "ì´ë…¸ë² ì´ì…˜",
            
            # ì¼ë°˜ì ì¸ ì¹´íƒ€ì¹´ë‚˜ ë‹¨ì–´ë“¤  
            "ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°": "ë§ˆì¼€íŒ…",
            "ã‚³ãƒ³ã‚µãƒ«ãƒ†ã‚£ãƒ³ã‚°": "ì»¨ì„¤íŒ…",
            "ã‚¨ãƒ³ã‚¿ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ": "ì—”í„°í…Œì¸ë¨¼íŠ¸",
            "ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ": "í”Œë«í¼",
            "ãƒ¡ãƒ‡ã‚£ã‚¢": "ë¯¸ë””ì–´",
            "ã‚²ãƒ¼ãƒ ": "ê²Œì„",
            "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„": "ì½˜í…ì¸ "
        }
        
        # 1. ë§¤í•‘ í…Œì´ë¸”ë¡œ ê¸°ë³¸ ë²ˆì—­ (ë¹ ë¥¸ ì²˜ë¦¬)
        translated = japanese_text
        for jp, ko in sorted(translation_map.items(), key=lambda x: len(x[0]), reverse=True):
            translated = translated.replace(jp, ko)
        
        # 2. ë§¤í•‘ í…Œì´ë¸”ë¡œë§Œ ì¶©ë¶„íˆ ë²ˆì—­ëœ ê²½ìš° (ì¼ë³¸ì–´ê°€ ê±°ì˜ ë‚¨ì§€ ì•ŠìŒ)
        # ì›ë³¸ê³¼ ë™ì¼í•˜ë©´ ë²ˆì—­ì´ ì•ˆëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ Google ë²ˆì—­ ì‹œë„
        if translated != japanese_text and not re.search(r'[ã²ã‚‰ãŒãªã‚«ã‚¿ã‚«ãƒŠæ¼¢å­—]', translated):
            translated = re.sub(r'\s+', ' ', translated).strip()
            logger.info(f"ë§¤í•‘ í…Œì´ë¸” ë²ˆì—­: {japanese_text} â†’ {translated}")
            return translated
        
        # 3. Google Translate API ì‚¬ìš© (ì¼ë³¸ì–´ê°€ ë‚¨ì•„ìˆëŠ” ê²½ìš°)
        try:
            # Google Translate API í˜¸ì¶œ
            google_translated = await self._call_google_translate(japanese_text)
            if google_translated:
                translated = google_translated 
                logger.info(f"Google ë²ˆì—­: {japanese_text} â†’ {translated}")
            else:
                # API ì‹¤íŒ¨ì‹œ ë§¤í•‘ í…Œì´ë¸” ê²°ê³¼ë¼ë„ ì‚¬ìš©
                translated = re.sub(r'\s+', ' ', translated).strip()
                logger.warning(f"Google ë²ˆì—­ ì‹¤íŒ¨, ë§¤í•‘ í…Œì´ë¸” ì‚¬ìš©: {japanese_text} â†’ {translated}")
        except Exception as e:
            logger.error(f"ë²ˆì—­ ì¤‘ ì˜¤ë¥˜: {e}, ë§¤í•‘ í…Œì´ë¸” ê²°ê³¼ ì‚¬ìš©")
            translated = re.sub(r'\s+', ' ', translated).strip()
        
        return translated.strip()
    
    async def _call_google_translate(self, text: str) -> Optional[str]:
        """Google Translate API í˜¸ì¶œ (REST API ì§ì ‘ ì‚¬ìš©)"""
        try:
            import aiohttp
            
            # API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            google_api_key = os.getenv("GOOGLE_TRANSLATE_API_KEY")
            if not google_api_key:
                logger.warning("GOOGLE_TRANSLATE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return None
            
            # Google Translate REST API í˜¸ì¶œ
            url = "https://translation.googleapis.com/language/translate/v2"
            params = {
                'key': google_api_key,
                'q': text,
                'source': 'ja',
                'target': 'ko',
                'format': 'text'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, data=params) as response:
                    if response.status == 200:
                        result = await response.json()
                        if 'data' in result and 'translations' in result['data']:
                            translated_text = result['data']['translations'][0]['translatedText']
                            return translated_text
                    else:
                        logger.error(f"Google Translate API ì˜¤ë¥˜: {response.status}")
                        return None
            
        except Exception as e:
            logger.error(f"Google Translate API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _get_market_cap(self, sec_code: str) -> Optional[int]:
        """Yahoo Financeì—ì„œ ì‹œê°€ì´ì•¡ ê°€ì ¸ì˜¤ê¸°"""
        if not sec_code or len(sec_code) != 4:
            return None
        
        try:
            import httpx
            
            url = f"https://finance.yahoo.co.jp/quote/{sec_code}.T"
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1"
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=headers)
                response.raise_for_status()
                
                content = response.text
                logger.debug(f"Yahoo Finance í˜ì´ì§€ í¬ê¸°: {len(content)} ë¬¸ì")
                
                # ì‹œê°€ì´ì•¡ íŒ¨í„´ - ìƒˆë¡œìš´ DOM êµ¬ì¡°ì— ë§ê²Œ ì—…ë°ì´íŠ¸  
                # êµ¬ì¡°: <span>æ™‚ä¾¡ç·é¡</span>...<dd>...<span class="StyledNumber__value__3rXW">VALUE</span>...<span>ç™¾ä¸‡å††</span>
                pattern = r'æ™‚ä¾¡ç·é¡.*?StyledNumber__value__[^>]*>([^<]+)</span>.*?ç™¾ä¸‡å††'
                
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    try:
                        value_str = match.group(1).replace(',', '').replace('.', '')
                        if value_str:
                            value = float(value_str) * 1_000_000  # ë°±ë§Œì—” â†’ ì—”
                            
                            # í•©ë¦¬ì ì¸ ë²”ìœ„ ì²´í¬ (1ì²œì–µì—” ~ 1,000ì¡°ì—”)
                            if 100_000_000_000 <= value <= 1_000_000_000_000_000:
                                logger.info(f"ì‹œê°€ì´ì•¡ ì¶”ì¶œ ì„±ê³µ: {sec_code} â†’ {int(value):,}ì—”")
                                return int(value)
                                
                    except (ValueError, IndexError):
                        pass
                
                logger.warning(f"ì‹œê°€ì´ì•¡ì„ ì°¾ì§€ ëª»í•¨: {sec_code}")
                
        except Exception as e:
            logger.error(f"ì‹œê°€ì´ì•¡ ì¶”ì¶œ ì‹¤íŒ¨ ({sec_code}): {e}")
        
        return None    
    def _extract_value_from_context(self, content: str, keyword: str, filename: str) -> Optional[float]:
        """í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ ìˆ˜ì¹˜ ì¶”ì¶œ"""
        try:
            # ì¢…ì—…ì›ìˆ˜ì˜ ê²½ìš° ë‹¨ìˆœíˆ ê°’ë§Œ ì¶”ì¶œ
            if "å¾“æ¥­å“¡æ•°" in keyword or "NumberOfEmployees" in keyword:
                return self._extract_employee_count(content, filename)
            
            # í‰ê·  ì—°ë´‰ì˜ ê²½ìš° ì •í™•í•œ ì› ë‹¨ìœ„ ê°’ ì°¾ê¸°  
            if "å¹³å‡å¹´é–“çµ¦ä¸" in keyword or "AverageAnnualSalary" in keyword:
                return self._extract_annual_salary(content, filename)
            
            # ê¸°íƒ€ í‚¤ì›Œë“œë“¤ (ê·¼ì†ì—°ìˆ˜, ë‚˜ì´)
            if "å¹³å‡å‹¤ç¶šå¹´æ•°" in keyword:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)\s*å¹´"
            elif "å¹³å‡å¹´é½¢" in keyword:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)\s*æ­³"
            else:
                pattern = rf"{keyword}[^0-9]*?([0-9]+\.?[0-9]*)"
            
            match = re.search(pattern, content)
            if match:
                raw_value = match.group(1).replace(',', '')
                value = float(raw_value)
                logger.info(f"í‚¤ì›Œë“œ '{keyword}'ì—ì„œ ê°’ ì¶”ì¶œ: {value} (íŒŒì¼: {filename})")
                return value
        except Exception as e:
            logger.error(f"ê°’ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _select_best_value_by_context(self, results: List[dict]) -> dict:
        """contextRef ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ ê°’ ì„ íƒ"""
        if not results:
            return None
            
        if len(results) == 1:
            return results[0]
        
        logger.info(f"ì—¬ëŸ¬ ê°’ ì¤‘ ìµœì  ì„ íƒ - ì´ {len(results)}ê°œ:")
        for i, result in enumerate(results):
            logger.info(f"  {i+1}: {result['value']} (contextRef: {result['contextRef']})")
        
        # ì„ íƒ ê¸°ì¤€:
        # 1. ì „ì²´/ì—°ê²° ê¸°ì¤€ ìš°ì„  (ì„¸ê·¸ë¨¼íŠ¸ ì œì™¸)
        # 2. ê°€ì¥ ìµœê·¼ ì—°ë„ (Current > Prior1 > Prior2...)
        # 3. ì—°ê²° ê¸°ì¤€ > ë‹¨ë… ê¸°ì¤€
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ ìš°ì„ ìˆœìœ„ ë¶„ë¥˜
        overall_results = []      # ì „ì²´/ì—°ê²° (ì„¸ê·¸ë¨¼íŠ¸ ì—†ìŒ)
        segment_results = []      # ì„¸ê·¸ë¨¼íŠ¸ë³„ 
        non_consol_results = []   # ë‹¨ë… ê¸°ì¤€
        
        for result in results:
            context_ref = result.get('contextRef') or ''
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë©¤ë²„ê°€ ìˆëŠ”ì§€ í™•ì¸ (ReportableSegment, CorporateShared ë“±)
            if any(segment in context_ref for segment in ['ReportableSegment', 'Member']):
                if 'NonConsolidated' in context_ref:
                    non_consol_results.append(result)
                else:
                    segment_results.append(result)
            else:
                # ì„¸ê·¸ë¨¼íŠ¸ê°€ ì—†ëŠ” ì „ì²´/ì—°ê²° ê¸°ì¤€
                overall_results.append(result)
        
        # ìš°ì„ ìˆœìœ„: ì „ì²´/ì—°ê²° > ì„¸ê·¸ë¨¼íŠ¸ > ë‹¨ë…
        candidates = overall_results if overall_results else (segment_results if segment_results else non_consol_results)
        
        if not candidates:
            candidates = results
        
        # ì»¨í…ìŠ¤íŠ¸ ìš°ì„ ìˆœìœ„ì— ë”°ë¥¸ ì„ íƒ
        def get_context_priority(context_ref):
            """contextRefì˜ ìš°ì„ ìˆœìœ„ë¥¼ ë°˜í™˜ (ë‚®ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)"""
            if 'CurrentYear' in context_ref:
                return 0  # ìµœê³  ìš°ì„ ìˆœìœ„
            elif 'Prior1Year' in context_ref:
                return 1
            elif 'Prior2Year' in context_ref:
                return 2
            elif 'Prior3Year' in context_ref:
                return 3
            elif 'Prior4Year' in context_ref:
                return 4
            else:
                return 999  # ì•Œ ìˆ˜ ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ëŠ” ë‚®ì€ ìš°ì„ ìˆœìœ„
        
        # ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ (ìˆ«ìê°€ ê°€ì¥ ë‚®ì€) ê²°ê³¼ ì„ íƒ
        best_result = min(candidates, key=lambda x: get_context_priority(x.get('contextRef', '')))
        
        logger.info(f"ìµœì¢… ì„ íƒ: {best_result['value']} (contextRef: {best_result['contextRef']})")
        return best_result
    
    def _apply_ixbrl_attributes(self, result: dict, field: str) -> float:
        """scale/decimals/unitRef ì†ì„±ì„ ì ìš©í•˜ì—¬ ìµœì¢… ê°’ ê³„ì‚°"""
        try:
            value = float(str(result['value']).replace(',', ''))
        except (ValueError, AttributeError):
            logger.warning(f"ê°’ ë³€í™˜ ì‹¤íŒ¨: {result['value']}")
            return 0.0
        
        scale = result.get('scale')
        decimals = result.get('decimals')
        unit_ref = result.get('unitRef')
        
        logger.info(f"ì†ì„± ì ìš© ì „ - ê°’: {value}, scale: {scale}, decimals: {decimals}, unit: {unit_ref}")
        
        # scale ì ìš© (10^scaleë¥¼ ê³±í•¨)
        if scale is not None:
            try:
                scale_factor = int(scale)
                value *= (10 ** scale_factor)
                logger.info(f"scale {scale} ì ìš©: {result['value']} â†’ {value}")
            except ValueError:
                logger.warning(f"ì˜ëª»ëœ scale ê°’: {scale}")
        
        # decimals ì ìš© (ìŒìˆ˜ë©´ scale íš¨ê³¼, ì–‘ìˆ˜ë©´ ì†Œìˆ˜ì  ìë¦¿ìˆ˜)
        if decimals is not None:
            try:
                decimal_places = int(decimals)
                if decimal_places < 0:
                    # ìŒìˆ˜ decimalsëŠ” 10ì˜ ì§€ìˆ˜ë¡œ ë‚˜ëˆ” (scaleê³¼ ë¹„ìŠ·í•œ íš¨ê³¼)
                    value /= (10 ** abs(decimal_places))
                    logger.info(f"decimals {decimals} ì ìš© (scale íš¨ê³¼): {result['value']} â†’ {value}")
                elif decimal_places >= 0:
                    # ì–‘ìˆ˜ decimalsëŠ” ì†Œìˆ˜ì  ìë¦¿ìˆ˜ë¡œ ë°˜ì˜¬ë¦¼
                    value = round(value, decimal_places)
                    logger.info(f"decimals {decimals} ì ìš© (ë°˜ì˜¬ë¦¼): ì†Œìˆ˜ì  {decimal_places}ìë¦¬")
            except ValueError:
                logger.warning(f"ì˜ëª»ëœ decimals ê°’: {decimals}")
        
        # unitRef ê²€ì¦ ë° ë¡œê¹…
        if unit_ref:
            if field == 'avgAnnualSalaryJPY' and 'JPY' not in unit_ref.upper():
                logger.warning(f"ì˜ˆìƒê³¼ ë‹¤ë¥¸ í†µí™” ë‹¨ìœ„: {unit_ref} (JPY ì˜ˆìƒ)")
            logger.info(f"ë‹¨ìœ„ í™•ì¸: {unit_ref}")
        
        # í•„ë“œë³„ íƒ€ì… ë³€í™˜
        if field in ['avgAnnualSalaryJPY', 'employeeCount']:
            value = int(value)  # ì •ìˆ˜ ë³€í™˜
        else:
            value = float(value)  # ì†Œìˆ˜ì  ìœ ì§€
        
        logger.info(f"ìµœì¢… ì ìš©ëœ ê°’: {value}")
        return value
    
    def _extract_employee_count(self, content: str, filename: str) -> Optional[float]:
        """ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ (ë‹¨ìˆœ ë°©ì‹)"""
        try:
            logger.info(f"ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ ì‹œì‘ - íŒŒì¼: {filename}")
            
            # ë‹¨ìˆœ ë°©ì‹: ê´‘ë²”ìœ„í•œ íŒ¨í„´ìœ¼ë¡œ ëª¨ë“  ì§ì›ìˆ˜ë¥¼ ì°¾ê³  ê°€ì¥ í° ê°’ ì„ íƒ
            patterns = [
                # ë‹¨ìˆœ ìˆ«ì + äºº íŒ¨í„´ (í° ìˆ«ìë§Œ)
                r"([0-9]{4,5}[,0-9]*)\s*äºº",
                # í…Œì´ë¸” í˜•íƒœ íŒ¨í„´  
                r"åˆè¨ˆ[^0-9]*?([0-9,]+)\s*äºº",
                r"è¨ˆ[^0-9]*?([0-9,]+)\s*äºº",
                # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤
                r"å¾“æ¥­å“¡æ•°[^0-9]*?([0-9,]+)\s*äºº",
                r"å¾“æ¥­å“¡[^0-9]*?([0-9,]+)\s*äºº",
                r"NumberOfEmployees[^0-9]*?([0-9,]+)",
            ]
            
            all_matches = []
            
            for i, pattern in enumerate(patterns):
                matches = re.findall(pattern, content, re.DOTALL)
                if matches:
                    logger.info(f"íŒ¨í„´ {i+1}ì—ì„œ ë§¤ì¹˜ ë°œê²¬: {matches[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ë¡œê·¸
                    all_matches.extend(matches)
            
            if all_matches:
                # ëª¨ë“  ë§¤ì¹˜ì—ì„œ ê°€ì¥ í° ê°’ ì°¾ê¸° (í•©ê³„ê°’)
                max_value = 0
                for match in all_matches:
                    try:
                        # ë¬¸ìì—´ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ
                        clean_match = re.sub(r'[^0-9]', '', str(match))
                        if clean_match:
                            value = float(clean_match)
                            if value > max_value and value > 100:  # 100ëª… ì´ìƒë§Œ
                                max_value = value
                    except:
                        continue
                
                if max_value > 0:
                    logger.info(f"ìµœì¢… ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ: {max_value} (íŒŒì¼: {filename})")
                    return max_value
            
            logger.warning(f"ì¢…ì—…ì›ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - íŒŒì¼: {filename}")
                    
        except Exception as e:
            logger.error(f"ì¢…ì—…ì›ìˆ˜ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _estimate_data_year(self, content: str, filename: str) -> int:
        """ë°ì´í„° ì—°ë„ ì¶”ì •"""
        # 1. íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì¶œ ì‹œë„ (2025-03-31 ê°™ì€ íŒ¨í„´)
        year_match = re.search(r'(\d{4})', filename)
        if year_match:
            year = int(year_match.group(1))
            if 2020 <= year <= 2030:  # í•©ë¦¬ì ì¸ ë²”ìœ„
                logger.info(f"íŒŒì¼ëª…ì—ì„œ ì—°ë„ ì¶”ì •: {year}")
                return year
        
        # 2. ë‚´ìš©ì—ì„œ ê°€ì¥ ìµœê·¼ ì—°ë„ ì°¾ê¸°
        years = re.findall(r'(20[2-3][0-9])[å¹´\s]', content)
        if years:
            recent_year = max([int(y) for y in years if 2020 <= int(y) <= 2030])
            logger.info(f"ë‚´ìš©ì—ì„œ ì—°ë„ ì¶”ì •: {recent_year}")
            return recent_year
            
        # 3. ê¸°ë³¸ê°’: í˜„ì¬ ì—°ë„ - 1 (ë³´ê³ ì„œëŠ” ë³´í†µ ì „ë…„ë„ ê¸°ì¤€)
        default_year = datetime.now().year - 1
        logger.info(f"ê¸°ë³¸ê°’ìœ¼ë¡œ ì—°ë„ ì¶”ì •: {default_year}")
        return default_year
    
    def _find_employee_data_year(self, content: str, employee_count: float) -> int:
        """ì‹¤ì œ ì§ì›ìˆ˜ì™€ ì—°ê²°ëœ ì—°ë„ ì°¾ê¸°"""
        try:
            # ì§ì›ìˆ˜ ê°’ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ì½¤ë§ˆ í¬í•¨/ë¯¸í¬í•¨ ëª¨ë‘ ê³ ë ¤)
            count_str = str(int(employee_count))
            count_with_comma = f"{int(employee_count):,}"
            
            logger.info(f"ì§ì›ìˆ˜ {employee_count}ì™€ ì—°ê²°ëœ ì—°ë„ ì°¾ê¸° ì‹œì‘")
            
            # í˜„ì¬ ì‹œì ì—ì„œ í•©ë¦¬ì í•œ ì—°ë„ ë²”ìœ„ (2020~2025)
            current_year = datetime.now().year
            valid_years = range(2020, current_year + 1)
            
            for year in sorted(valid_years, reverse=True):  # ìµœê·¼ ì—°ë„ë¶€í„° ì°¾ê¸°
                # í•´ë‹¹ ì—°ë„ì™€ ì§ì›ìˆ˜ê°€ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ” íŒ¨í„´ë“¤ ì°¾ê¸°
                patterns = [
                    rf"{year}[å¹´\s].*?{count_str}\s*äºº",
                    rf"{year}[å¹´\s].*?{count_with_comma}\s*äºº",
                    rf"{year}.*?{count_str}\s*äºº",
                    rf"{year}.*?{count_with_comma}\s*äºº",
                    # í…Œì´ë¸” í˜•íƒœì—ì„œ ë…„ë„ì™€ ì§ì›ìˆ˜ê°€ ê°™ì€ í–‰ì— ìˆëŠ” ê²½ìš°
                    rf"{year}[^\n]*{count_str}[^\n]*äºº",
                    rf"{year}[^\n]*{count_with_comma}[^\n]*äºº"
                ]
                
                for pattern in patterns:
                    if re.search(pattern, content, re.DOTALL):
                        logger.info(f"ì§ì›ìˆ˜ {employee_count}ê°€ {year}ë…„ê³¼ ì—°ê²°ë¨")
                        return year
            
            # ì§ì ‘ì ì¸ ì—°ê²°ì´ ì—†ìœ¼ë©´ ìµœì‹  ì—°ë„ (2025ë…„ ë˜ëŠ” 2024ë…„)
            default_year = current_year  # ìµœì‹  ì—°ë„ë¡œ ì„¤ì •
            logger.info(f"ì—°ê²°ëœ ì—°ë„ ì—†ìŒ, ìµœì‹  ì—°ë„ë¡œ ì„¤ì •: {default_year}")
            return default_year
                    
        except Exception as e:
            logger.error(f"ì—°ë„ ì°¾ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            return datetime.now().year  # ìµœì‹  ì—°ë„
    
    def _extract_annual_salary(self, content: str, filename: str) -> Optional[float]:
        """í‰ê·  ì—°ë´‰ ì¶”ì¶œ (ì—” ë‹¨ìœ„)"""
        try:
            # 11,453,407 ê°™ì€ ì •í™•í•œ ì—” ë‹¨ìœ„ ê°’ ì°¾ê¸°
            patterns = [
                r"å¹³å‡å¹´é–“çµ¦ä¸[^0-9]*?([0-9,]+)\s*å††",  # ì¼ë³¸ì–´ + å††
                r"AverageAnnualSalary[^0-9]*?([0-9,]+)",  # ì˜ë¬¸
                r"å¹³å‡å¹´é–“çµ¦ä¸[^0-9]*?([0-9,]+)\s*ä¸‡å††", # ë§Œì› ë‹¨ìœ„
            ]
            
            for pattern in patterns:
                match = re.search(pattern, content)
                if match:
                    raw_value = match.group(1).replace(',', '')
                    value = float(raw_value)
                    
                    # ë§Œì—” ë‹¨ìœ„ì¸ ê²½ìš° ì—”ìœ¼ë¡œ ë³€í™˜
                    if "ä¸‡å††" in pattern:
                        value *= 10000
                    
                    logger.info(f"ì—°ë´‰ ì¶”ì¶œ: {value} (íŒŒì¼: {filename}, íŒ¨í„´: {pattern})")
                    return value
                    
        except Exception as e:
            logger.error(f"ì—°ë´‰ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _extract_value_from_ixbrl_concept(self, content: str, concept_name: str, filename: str) -> Optional[List[dict]]:
        """iXBRL ê°œë…(name ì†ì„±)ì—ì„œ ìˆ˜ì¹˜ ì¶”ì¶œ"""
        try:
            # ì „ì²´ ix:nonFraction íƒœê·¸ ë§¤ì¹­ (ì†ì„±ê³¼ ê°’ ëª¨ë‘ í¬í•¨)
            full_pattern = rf'<ix:nonFraction([^>]*name="{re.escape(concept_name)}"[^>]*)>([^<]+)</ix:nonFraction>'
            full_matches = re.findall(full_pattern, content)
            
            if full_matches:
                logger.info(f"ê°œë… '{concept_name}'ì—ì„œ {len(full_matches)}ê°œ ê°’ ë°œê²¬: {[m[1] for m in full_matches[:3]]}...")
                
                results = []
                for attrs, value_text in full_matches:
                    try:
                        # ìˆ«ì ê°’ ì¶”ì¶œ (ì½¤ë§ˆ ì œê±°)
                        clean_value = re.sub(r'[,\s]', '', value_text)
                        if not re.match(r'^-?[\d.]+$', clean_value):
                            continue
                            
                        value = float(clean_value)
                        
                        # ì†ì„±ë“¤ íŒŒì‹± (ì „ì²´ ì†ì„± ë¬¸ìì—´ì—ì„œ ì¶”ì¶œ)
                        result = {
                            'value': value,
                            'raw_text': value_text,
                            'contextRef': self._extract_attribute(attrs, 'contextRef'),
                            'unitRef': self._extract_attribute(attrs, 'unitRef'),
                            'scale': self._extract_attribute(attrs, 'scale'),
                            'decimals': self._extract_attribute(attrs, 'decimals'),
                            'concept': concept_name,
                            'file': filename
                        }
                        results.append(result)
                        
                        # ë””ë²„ê¹…ìš© ë¡œê·¸
                        logger.debug(f"ì¶”ì¶œëœ ê°’: {value}, contextRef: {result['contextRef']}, unitRef: {result['unitRef']}")
                        
                    except ValueError as e:
                        logger.debug(f"ê°’ ë³€í™˜ ì‹¤íŒ¨: {value_text} - {e}")
                        continue
                
                if results:
                    logger.info(f"ê°œë… '{concept_name}'ì—ì„œ {len(results)}ê°œ ìœ íš¨í•œ ê°’ ì¶”ì¶œ")
                    return results
                    
        except Exception as e:
            logger.error(f"iXBRL ê°œë… ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return None
    
    def _extract_attribute(self, attrs_string: str, attr_name: str) -> Optional[str]:
        """ì†ì„± ë¬¸ìì—´ì—ì„œ íŠ¹ì • ì†ì„±ê°’ ì¶”ì¶œ"""
        try:
            pattern = rf'{attr_name}="([^"]*)"'
            match = re.search(pattern, attrs_string)
            return match.group(1) if match else None
        except:
            return None



class CompanyReportUpdater:
    """ê¸°ì—… ë¦¬í¬íŠ¸ ìµœì‹ í™” ê´€ë¦¬ì"""
    
    def __init__(self):
        # íƒ€ê²Ÿ ê¸°ì—…ë“¤ê³¼ ë§¤ì¹­ í‚¤ì›Œë“œ (ëª¨ë‘ ë³¸ì‚¬ë§Œ ì •í™•íˆ ë§¤ì¹­)
        self.target_companies = {
            "rakuten": ["æ¥½å¤©ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"],
            "mercari": ["æ ªå¼ä¼šç¤¾ãƒ¡ãƒ«ã‚«ãƒª"],  
            "cyberagent": ["æ ªå¼ä¼šç¤¾ã‚µã‚¤ãƒãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"],
            "lineyahoo": ["ï¼¬ï¼©ï¼®ï¼¥ãƒ¤ãƒ•ãƒ¼æ ªå¼ä¼šç¤¾"],
            "recruit": ["æ ªå¼ä¼šç¤¾ãƒªã‚¯ãƒ«ãƒ¼ãƒˆãƒ›ãƒ¼ãƒ«ãƒ‡ã‚£ãƒ³ã‚°ã‚¹"],
            "dena": ["æ ªå¼ä¼šç¤¾ãƒ‡ã‚£ãƒ¼ãƒ»ã‚¨ãƒŒãƒ»ã‚¨ãƒ¼"],
            "sony": ["ã‚½ãƒ‹ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—æ ªå¼ä¼šç¤¾"],
            "softbank": ["ã‚½ãƒ•ãƒˆãƒãƒ³ã‚¯æ ªå¼ä¼šç¤¾"],
            "fujitsu": ["å¯Œå£«é€šæ ªå¼ä¼šç¤¾"],  # ë³¸ì‚¬ë¡œ ë³€ê²½
            "nttdata": ["æ ªå¼ä¼šç¤¾ï¼®ï¼´ï¼´ãƒ‡ãƒ¼ã‚¿ã‚°ãƒ«ãƒ¼ãƒ—"]
        }
        
        # ìƒíƒœ ì €ì¥ íŒŒì¼ ê²½ë¡œ
        self.state_file = Path("data/last_check_dates.json")
        self.discovered_reports = {}  # company_key -> ìµœì‹  ë¬¸ì„œ ì •ë³´
    
    def load_last_check_dates(self) -> Dict[str, str]:
        """ë§ˆì§€ë§‰ ì²´í¬ ë‚ ì§œ ë¡œë“œ"""
        try:
            if self.state_file.exists():
                with open(self.state_file, 'r') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"ìƒíƒœ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ê°’: 18ê°œì›” ì „ë¶€í„° ì‹œì‘
        default_date = (datetime.now() - timedelta(days=18*30)).strftime("%Y-%m-%d")
        return {company_key: default_date for company_key in self.target_companies.keys()}
    
    def save_last_check_dates(self, dates: Dict[str, str]):
        """ë§ˆì§€ë§‰ ì²´í¬ ë‚ ì§œ ì €ì¥"""
        try:
            self.state_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.state_file, 'w') as f:
                json.dump(dates, f, indent=2)
        except Exception as e:
            logger.error(f"ìƒíƒœ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def match_company(self, company_name: str) -> Optional[str]:
        """íšŒì‚¬ëª…ìœ¼ë¡œ íƒ€ê²Ÿ ê¸°ì—… ë§¤ì¹­"""
        for company_key, keywords in self.target_companies.items():
            for keyword in keywords:
                if keyword in company_name:
                    return company_key
        return None
    
    def date_range(self, start_date: datetime, end_date: datetime):
        """ë‚ ì§œ ë²”ìœ„ ìƒì„± (ì—­ìˆœ)"""
        current_date = end_date
        while current_date >= start_date:
            yield current_date.strftime("%Y-%m-%d")
            current_date -= timedelta(days=1)
    
    async def scan_date_for_reports(self, date: str, api: EdinetAPI) -> List[CompanyDocument]:
        """íŠ¹ì • ë‚ ì§œì—ì„œ íƒ€ê²Ÿ ê¸°ì—…ë“¤ì˜ ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ê²€ìƒ‰"""
        documents = await api.get_document_list(date)
        found_reports = []
        
        for doc in documents:
            # ìœ ê°€ì¦ê¶Œë³´ê³ ì„œë§Œ (docTypeCode: 120)
            if doc.get("docTypeCode") != "120":
                continue
            
            company_name = doc.get("filerName", "")
            company_key = self.match_company(company_name)
            
            if company_key:
                report = CompanyDocument(
                    document_id=doc.get("docID"),
                    company_name=company_name,
                    submitted_date=date,
                    doc_type="120",
                    company_key=company_key,
                    sec_code=doc.get("secCode")[:4] if doc.get("secCode") else None  # EDINET API ì‘ë‹µì—ì„œ secCode ì• 4ìë¦¬ë§Œ ì¶”ê°€
                )
                found_reports.append(report)
                logger.info(f"ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ë°œê²¬: {company_name} ({date}) - {doc.get('docID')}")
        
        return found_reports
    
    async def find_latest_reports(self, months_back: int = 18) -> Dict[str, CompanyDocument]:
        """ìµœì‹  ìœ ê°€ì¦ê¶Œë³´ê³ ì„œë“¤ ê²€ìƒ‰"""
        logger.info(f"ìµœê·¼ {months_back}ê°œì›”ê°„ ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ê²€ìƒ‰ ì‹œì‘...")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        end_date = datetime.now()
        start_date = end_date - timedelta(days=months_back * 30)
        
        # ê° íšŒì‚¬ë³„ ìµœì‹  ë¦¬í¬íŠ¸ ì €ì¥
        latest_reports = {}
        dates_scanned = 0
        
        async with EdinetAPI() as api:
            # ë‚ ì§œë³„ ìŠ¤ìº” (ìµœì‹  ë‚ ì§œë¶€í„°)
            for date_str in self.date_range(start_date, end_date):
                dates_scanned += 1
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if dates_scanned % 30 == 0:
                    logger.info(f"ì§„í–‰ë¥ : {dates_scanned}ì¼ ìŠ¤ìº” ì™„ë£Œ ({date_str})")
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë¦¬í¬íŠ¸ë“¤ ê²€ìƒ‰
                found_reports = await self.scan_date_for_reports(date_str, api)
                
                # ê° íšŒì‚¬ë³„ë¡œ ê°€ì¥ ìµœê·¼ ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸
                for report in found_reports:
                    company_key = report.company_key
                    
                    # ë” ìµœì‹  ë¦¬í¬íŠ¸ì´ê±°ë‚˜ ì²˜ìŒ ë°œê²¬í•œ ê²½ìš°
                    if (company_key not in latest_reports or 
                        report.submitted_date > latest_reports[company_key].submitted_date):
                        
                        latest_reports[company_key] = report
                        logger.info(f"âœ¨ {company_key} ìµœì‹  ë¦¬í¬íŠ¸ ì—…ë°ì´íŠ¸: {report.submitted_date}")
                
                # ëª¨ë“  íšŒì‚¬ì˜ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì•˜ìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
                if len(latest_reports) == len(self.target_companies):
                    logger.info(f"ğŸ‰ ëª¨ë“  íšŒì‚¬ì˜ ë¦¬í¬íŠ¸ ë°œê²¬! {dates_scanned}ì¼ ìŠ¤ìº”ìœ¼ë¡œ ì™„ë£Œ")
                    break
                
                # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                await asyncio.sleep(0.1)
        
        # ê²°ê³¼ ìš”ì•½
        logger.info(f"\nğŸ“Š ìµœì‹ í™” ê²°ê³¼ ìš”ì•½:")
        logger.info(f"- ìŠ¤ìº” ê¸°ê°„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        logger.info(f"- ìŠ¤ìº” ì¼ìˆ˜: {dates_scanned}ì¼")
        logger.info(f"- ë°œê²¬í•œ íšŒì‚¬: {len(latest_reports)}/{len(self.target_companies)}ê°œ")
        
        for company_key, report in latest_reports.items():
            logger.info(f"  â€¢ {company_key}: {report.company_name} ({report.submitted_date})")
        
        # ëª» ì°¾ì€ íšŒì‚¬ë“¤
        missing_companies = set(self.target_companies.keys()) - set(latest_reports.keys())
        if missing_companies:
            logger.warning(f"âŒ ë¦¬í¬íŠ¸ë¥¼ ì°¾ì§€ ëª»í•œ íšŒì‚¬ë“¤: {missing_companies}")
        
        return latest_reports
    
    async def update_company_data(self, company_key: str, document: CompanyDocument) -> bool:
        """íŠ¹ì • íšŒì‚¬ì˜ ë°ì´í„° ì—…ë°ì´íŠ¸"""
        logger.info(f"ğŸ”„ {company_key} ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹œì‘...")
        
        try:
            async with EdinetAPI() as api:
                # ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ
                zip_content = await api.get_document_package(document.document_id, doc_type="1")
                
                if zip_content:
                    # honbun íŒŒì¼ ì¶”ì¶œ ë° íŒŒì‹±
                    honbun_files = api.extract_honbun_files(zip_content)
                    
                    if honbun_files:
                        # ì¸ì‚¬ ì •ë³´ ë° ê¸°ë³¸ ì •ë³´ íŒŒì‹±
                        employee_info = api.parse_employee_info(honbun_files)
                        basic_info = api.parse_basic_info(honbun_files, zip_content)
                        
                        # EdinetData ê°ì²´ ìƒì„±
                        edinet_data = EdinetData()
                        
                        # ê¸°ë³¸ ì •ë³´ ì„¤ì •
                        edinet_data.basic = EdinetBasic()
                        edinet_data.basic.name = basic_info.get("name") or document.company_name
                        edinet_data.basic.name_en = basic_info.get("name_en", "")
                        edinet_data.basic.name_ko = await api._translate_to_korean(edinet_data.basic.name) if edinet_data.basic.name else ""
                        edinet_data.basic.headquarters = basic_info.get("headquarters", "")  # ì¼ë³¸ì–´ ì›ë³¸
                        # ë³¸ì‚¬ ì£¼ì†Œ í•œê¸€ ë²ˆì—­
                        edinet_data.basic.headquarters_ko = await api._translate_to_korean(edinet_data.basic.headquarters) if edinet_data.basic.headquarters else ""
                        # ë³¸ì‚¬ ì£¼ì†Œ ì˜ë¬¸ ë²ˆì—­ (ì¶”í›„ êµ¬í˜„ ê°€ëŠ¥)
                        edinet_data.basic.headquarters_en = ""  # í˜„ì¬ëŠ” ë¹ˆ ê°’
                        # founded_year ì§ì ‘ ì„¤ì •
                        edinet_data.basic.founded_year = basic_info.get("founded_year")
                        # CompanyDocumentì—ì„œ secCodeê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ ë¬¸ì„œì—ì„œ ì¶”ì¶œí•œ ê²ƒ ì‚¬ìš©
                        edinet_data.basic.sec_code = document.sec_code or basic_info.get("sec_code")
                        
                        # ì‹œê°€ì´ì•¡ ê°€ì ¸ì˜¤ê¸° (ìƒì¥ë²ˆí˜¸ê°€ ìˆëŠ” ê²½ìš°)
                        if edinet_data.basic.sec_code:
                            market_cap = await api._get_market_cap(edinet_data.basic.sec_code)
                            edinet_data.basic.market_cap = market_cap
                        edinet_data.basic.employee_count = employee_info.get("employeeCount")
                        
                        # HR ì •ë³´ ì„¤ì •
                        edinet_data.hr.avgTenureYears = employee_info.get("avgTenureYears")
                        edinet_data.hr.avgAgeYears = employee_info.get("avgAgeYears")
                        edinet_data.hr.avgAnnualSalaryJPY = employee_info.get("avgAnnualSalaryJPY")
                        
                        # ì¬ë¬´ ì •ë³´ ì„¤ì •
                        edinet_data.financials = EdinetFinancials()
                        edinet_data.financials.fiscalYear = datetime.now().year
                        
                        # ì¶œì²˜ ì •ë³´ ì„¤ì •
                        edinet_data.provenance = {
                            "source": "EDINET API v2",
                            "document_id": document.document_id,
                            "submitted_date": document.submitted_date,
                            "fetched_at": datetime.now().isoformat(),
                            "company_key": company_key,
                            "employee_info_provenance": employee_info.get("provenance", {})
                        }
                        
                        # ë°ì´í„° ì €ì¥
                        await self.save_company_data(company_key, edinet_data)
                        
                        logger.info(f"âœ… {company_key} ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
                        return True
                    else:
                        logger.error(f"âŒ {company_key} honbun íŒŒì¼ ì¶”ì¶œ ì‹¤íŒ¨")
                else:
                    logger.error(f"âŒ {company_key} ë¬¸ì„œ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    
        except Exception as e:
            logger.error(f"âŒ {company_key} ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return False
    
    async def save_company_data(self, company_key: str, edinet_data: EdinetData):
        """íšŒì‚¬ ë°ì´í„° ì €ì¥"""
        output_path = Path(f"data/edinet_reports/{company_key}.json")
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # EdinetDataë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(edinet_data.model_dump(), f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"ğŸ’¾ {company_key} ë°ì´í„° ì €ì¥: {output_path}")
    
    async def run_full_update(self) -> Dict[str, bool]:
        """ì „ì²´ ìµœì‹ í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        logger.info("ğŸš€ EDINET ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ ìµœì‹ í™” ì‹œì‘...")
        
        # 1. ìµœì‹  ë¦¬í¬íŠ¸ë“¤ ê²€ìƒ‰
        latest_reports = await self.find_latest_reports()
        
        if not latest_reports:
            logger.warning("ê²€ìƒ‰ëœ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        # 2. ê° íšŒì‚¬ë³„ ë°ì´í„° ì—…ë°ì´íŠ¸
        update_results = {}
        
        for company_key, document in latest_reports.items():
            logger.info(f"\nğŸ“‹ {company_key} ì²˜ë¦¬ ì¤‘...")
            success = await self.update_company_data(company_key, document)
            update_results[company_key] = success
            
            # API ë¶€í•˜ ë°©ì§€
            await asyncio.sleep(2.0)
        
        # 3. ê²°ê³¼ ìš”ì•½
        successful_updates = sum(update_results.values())
        logger.info(f"\nğŸŠ ìµœì‹ í™” ì™„ë£Œ!")
        logger.info(f"ì„±ê³µ: {successful_updates}/{len(update_results)}ê°œ íšŒì‚¬")
        
        # ì„±ê³µí•œ íšŒì‚¬ë“¤
        for company_key, success in update_results.items():
            status = "âœ…" if success else "âŒ"
            logger.info(f"  {status} {company_key}")
        
        return update_results


async def fetch_edinet_data(company_code: str) -> EdinetData:
    """EDINETì—ì„œ ê¸°ì—… ë°ì´í„° ì¢…í•© ìˆ˜ì§‘"""
    edinet_data = EdinetData()
    
    async with EdinetAPI() as api:
        # 1. ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ document_idë¡œ ì²˜ë¦¬)
        zip_content = await api.get_document_package(company_code)
        if zip_content:
            # 2. honbun íŒŒì¼ë“¤ ì¶”ì¶œ
            honbun_files = api.extract_honbun_files(zip_content)
            
            if honbun_files:
                # 3. ê¸°ë³¸ ì •ë³´ íŒŒì‹± (ì„¤ë¦½ë…„ë„, íšŒì‚¬ëª…, ë³¸ì‚¬ ì£¼ì†Œ ë“±)
                basic_info = api.parse_basic_info(honbun_files, zip_content)
                
                # 4. ì¸ì‚¬ ì •ë³´ íŒŒì‹±
                employee_info = api.parse_employee_info(honbun_files)
                
                # 5. EdinetBasic ê°ì²´ì— ì €ì¥
                edinet_data.basic.name = basic_info.get("name", "")
                edinet_data.basic.name_en = basic_info.get("name_en", "")
                edinet_data.basic.headquarters = basic_info.get("headquarters", "")
                edinet_data.basic.founded_year = basic_info.get("founded_year")
                edinet_data.basic.sec_code = basic_info.get("sec_code")
                edinet_data.basic.employee_count = employee_info["employeeCount"]
                
                # 6. EdinetHR ê°ì²´ì— ì €ì¥
                edinet_data.hr.avgTenureYears = employee_info["avgTenureYears"]
                edinet_data.hr.avgAgeYears = employee_info["avgAgeYears"]
                edinet_data.hr.avgAnnualSalaryJPY = employee_info["avgAnnualSalaryJPY"]
                
                # 7. ì¶œì²˜ ì •ë³´ ì €ì¥
                edinet_data.provenance = {
                    "source": "EDINET API v2",
                    "company_code": company_code,
                    "fetched_at": datetime.now().isoformat(),
                    "basic_info_provenance": basic_info.get("provenance", {}),
                    "employee_info_provenance": employee_info.get("provenance", {})
                }
            else:
                logger.warning("honbun íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.warning("ìœ ê°€ì¦ê¶Œë³´ê³ ì„œ íŒ¨í‚¤ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì¬ë¬´ ì •ë³´ ì„¤ì •
    edinet_data.financials = EdinetFinancials()
    edinet_data.financials.fiscalYear = datetime.now().year
    
    return edinet_data

# í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜
async def test_edinet_api():
    """EDINET API í…ŒìŠ¤íŠ¸"""
    company_code = "S100VZG5"  # ë¦¬ì¿ ë¥´íŠ¸ í™€ë”©ìŠ¤
    
    print("EDINET API í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    data = await fetch_edinet_data(company_code)
    
    print(f"ê¸°ì—…ëª…: {data.basic.name}")
    print(f"ë³¸ì : {data.basic.headquarters}")
    print(f"ì§ì› ìˆ˜: {data.basic.employee_count}")
    print(f"í‰ê·  ê·¼ì†ì—°ìˆ˜: {data.hr.avgTenureYears}")
    print(f"í‰ê·  ì—°ë ¹: {data.hr.avgAgeYears}")
    print(f"í‰ê·  ì—°ë´‰: {data.hr.avgAnnualSalaryJPY}")
    print(f"ì¶œì²˜: {data.provenance}")
    
    return data

# ìµœì‹ í™” ì‹¤í–‰ í•¨ìˆ˜
async def run_edinet_update():
    """EDINET ìµœì‹ í™” ì‹¤í–‰"""
    updater = CompanyReportUpdater()
    results = await updater.run_full_update()
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "update":
        # python -m src.jap_syu.utils.edinet update
        asyncio.run(run_edinet_update())
    else:
        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_edinet_api())
